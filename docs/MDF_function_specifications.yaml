MatMul:
    description: Matrix multiplication (work in progress...)
    arguments:
    - A
    - B
    expression_string: A @ B
Relu:
    description: Rectified linear function (work in progress...)
    arguments:
    - A
    expression_string: A * (A > 0)
change_goal:
    description: Modifies the current goal buffer using the given pattern.
    arguments:
    - pattern
    - curr_goal
    expression_string: change_goal(pattern,curr_goal)
check_termination:
    description: Function used to check if no production was selected.
    arguments:
    - production
    expression_string: check_termination(production)
chunk_to_string:
    description: Converts a chunk dictionary to a string format.
    arguments:
    - chunk
    expression_string: chunk_to_string(chunk)
conflict_resolution_function:
    description: 'ACT-R conflict resolution function. Currently selects a production
        at random from the already matched productions, since utility values and learning

        are not implemented yet.'
    arguments:
    - productions
    expression_string: conflict_resolution_function(productions)
cos:
    description: Cosine function
    arguments:
    - variable0
    - scale
    expression_string: scale * cos(variable0)
cosh:
    description: Hyperbolic cosine function
    arguments:
    - variable0
    - scale
    expression_string: scale * cosh(variable0)
drift_diffusion_integrator:
    description: 'Integrates the drift diffusion model for a single trial using and
        implementation of the using the Euler-Maruyama method. This is a proof of
        concept implementation and

        is not optimized for speed.'
    arguments:
    - starting_point
    - non_decision_time
    - drift_rate
    - threshold
    - noise
    - dt
    expression_string: drift_diffusion_integrator(starting_point,non_decision_time,drift_rate,threshold,noise,dt)
exponential:
    description: Exponential function
    arguments:
    - variable0
    - scale
    - rate
    - bias
    - offset
    expression_string: scale * exp((rate * variable0) + bias) + offset
linear:
    description: A linear function, calculated from a slope and an intercept
    arguments:
    - variable0
    - slope
    - intercept
    expression_string: (variable0 * slope + intercept)
logistic:
    description: Logistic function
    arguments:
    - variable0
    - gain
    - bias
    - offset
    expression_string: 1/(1 + exp(-1*gain*(variable0 + bias) + offset))
match_production:
    description: Returns True if the production's left hand side matches the given
        context and adds the matching bindings to the production.
    arguments:
    - production
    - context
    expression_string: match_production(production,context)
onnx::Abs:
    description: '

        Absolute takes one input data (Tensor<T>) and produces one output data

        (Tensor<T>) where the absolute is, y = abs(x), is applied to

        the tensor elementwise.

        '
    arguments:
    - X
    expression_string: onnx_ops.abs(X)
onnx::Acos:
    description: '

        Calculates the arccosine (inverse of cosine) of the given input tensor, element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.acos(input)
onnx::Acosh:
    description: '

        Calculates the hyperbolic arccosine of the given input tensor element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.acosh(input)
onnx::Add:
    description: '

        Performs element-wise binary addition (with Numpy-style broadcasting support).


        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).


        (Opset 14 change): Extend supported types to include uint8, int8, uint16,
        and int16.

        '
    arguments:
    - A
    - B
    expression_string: onnx_ops.add(A, B)
onnx::And:
    description: '

        Returns the tensor resulted from performing the `and` logical operation

        elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting
        support).


        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).

        '
    arguments:
    - A
    - B
    expression_string: onnx_ops.and(A, B)
    function: null
onnx::ArgMax:
    description: '

        Computes the indices of the max elements of the input tensor''s element along
        the

        provided axis. The resulting tensor has the same rank as the input if keepdims
        equals 1.

        If keepdims equals 0, then the resulting tensor has the reduced dimension
        pruned.

        If select_last_index is True (default False), the index of the last occurrence
        of the max

        is selected if the max appears more than once in the input. Otherwise the
        index of the

        first occurrence is selected.

        The type of the output tensor is integer.'
    arguments:
    - data
    expression_string: onnx_ops.argmax(data, axis, keepdims, select_last_index)
onnx::ArgMin:
    description: '

        Computes the indices of the min elements of the input tensor''s element along
        the

        provided axis. The resulting tensor has the same rank as the input if keepdims
        equals 1.

        If keepdims equals 0, then the resulting tensor has the reduced dimension
        pruned.

        If select_last_index is True (default False), the index of the last occurrence
        of the min

        is selected if the min appears more than once in the input. Otherwise the
        index of the

        first occurrence is selected.

        The type of the output tensor is integer.'
    arguments:
    - data
    expression_string: onnx_ops.argmin(data, axis, keepdims, select_last_index)
onnx::Asin:
    description: '

        Calculates the arcsine (inverse of sine) of the given input tensor, element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.asin(input)
onnx::Asinh:
    description: '

        Calculates the hyperbolic arcsine of the given input tensor element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.asinh(input)
onnx::Atan:
    description: '

        Calculates the arctangent (inverse of tangent) of the given input tensor,
        element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.atan(input)
onnx::Atanh:
    description: '

        Calculates the hyperbolic arctangent of the given input tensor element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.atanh(input)
onnx::AveragePool:
    description: "\n AveragePool consumes an input tensor X and applies average pooling\
        \ across\n the tensor according to kernel sizes, stride sizes, and pad lengths.\n\
        \ average pooling consisting of computing the average on all values of a\n\
        \ subset of the input tensor according to the kernel size and downsampling\
        \ the\n data into the output tensor Y for further processing. The output spatial\
        \ shape will be following:\n ```\n output_spatial_shape[i] = floor((input_spatial_shape[i]\
        \ + pad_shape[i] - kernel_spatial_shape[i]) / strides_spatial_shape[i] + 1)\n\
        \ ```\n or\n ```\n output_spatial_shape[i] = ceil((input_spatial_shape[i]\
        \ + pad_shape[i] - kernel_spatial_shape[i]) / strides_spatial_shape[i] + 1)\n\
        \ ```\n if ceil_mode is enabled\n\n ```\n * pad_shape[i] is sum of pads along\
        \ axis i\n ```\n\n `auto_pad` is a DEPRECATED attribute. If you are using\
        \ them currently, the output spatial shape will be following:\n ```\n VALID:\
        \ output_spatial_shape[i] = ceil((input_spatial_shape[i] - kernel_spatial_shape[i]\
        \ + 1) / strides_spatial_shape[i])\n SAME_UPPER or SAME_LOWER: output_spatial_shape[i]\
        \ = ceil(input_spatial_shape[i] / strides_spatial_shape[i])\n ```\n And pad\
        \ shape will be following if `SAME_UPPER` or `SAME_LOWER`:\n ```\n pad_shape[i]\
        \ = (output_spatial_shape[i] - 1) * strides_spatial_shape[i] + kernel_spatial_shape[i]\
        \ - input_spatial_shape[i]\n ```\n The output of each pooling window is divided\
        \ by the number of elements (exclude pad when attribute count_include_pad\
        \ is zero).\n "
    arguments:
    - X
    expression_string: onnx_ops.averagepool(X, auto_pad, ceil_mode, count_include_pad,
        kernel_shape, pads, strides)
onnx::BatchNormalization:
    description: '

        Carries out batch normalization as described in the paper

        https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,

        There are five required inputs ''X'', ''scale'', ''B'', ''input_mean'' and

        ''input_var''.

        Note that ''input_mean'' and ''input_var'' are expected to be the estimated

        statistics in inference mode (training_mode=False, default),

        and the running statistics in training mode (training_mode=True).

        There are multiple cases for the number of outputs, which we list below:


        Output case #1: Y, running_mean, running_var (training_mode=True)

        Output case #2: Y (training_mode=False)


        When training_mode=False, extra outputs are invalid.

        The outputs are updated as follows when training_mode=True:

        ```

        running_mean = input_mean * momentum + current_mean * (1 - momentum)

        running_var = input_var * momentum + current_var * (1 - momentum)


        Y = (X - current_mean) / sqrt(current_var + epsilon) * scale + B


        where:


        current_mean = ReduceMean(X, axis=all_except_channel_index)

        current_var =  ReduceVar(X, axis=all_except_channel_index)


        Notice that ReduceVar refers to the population variance, and it equals to

        sum(sqrd(x_i - x_avg)) / N

        where N is the population size (this formula does not use sample size N -
        1).


        ```


        The computation of ReduceMean and ReduceVar uses float to avoid overflow for
        float16 inputs.


        When training_mode=False:

        ```

        Y = (X - input_mean) / sqrt(input_var + epsilon) * scale + B

        ```


        For previous (depreciated) non-spatial cases, implementors are suggested

        to flatten the input shape to (N x C * D1 * D2 * ... * Dn) before a BatchNormalization
        Op.

        This operator has **optional** inputs/outputs. See [the doc](IR.md) for more
        details about the representation of optional arguments. An empty string may
        be used in the place of an actual argument''s name to indicate a missing argument.
        Trailing optional arguments (those not followed by an argument that is present)
        may also be simply omitted.

        '
    arguments:
    - X
    - scale
    - B
    - input_mean
    - input_var
    expression_string: onnx_ops.batchnormalization(X, scale, B, input_mean, input_var,
        epsilon, momentum, training_mode)
onnx::Bernoulli:
    description: '

        Draws binary random numbers (0 or 1) from a Bernoulli distribution. The input
        tensor should be a tensor

        containing probabilities p (a value in the range [0,1]) to be used for drawing
        the binary random number,

        where an output of 1 is produced with probability p and an output of 0 is
        produced with probability (1-p).


        This operator is non-deterministic and may not produce the same values in
        different

        implementations (even if a seed is specified).

        '
    arguments:
    - input
    expression_string: onnx_ops.bernoulli(input, dtype, seed)
onnx::BitShift:
    description: "\nBitwise shift operator performs element-wise operation. For each\
        \ input element, if the\n attribute \"direction\" is \"RIGHT\", this operator\
        \ moves its binary representation toward\n the right side so that the input\
        \ value is effectively decreased. If the attribute \"direction\"\n is \"LEFT\"\
        , bits of binary representation moves toward the left side, which results\
        \ the\n increase of its actual value. The input X is the tensor to be shifted\
        \ and another input\n Y specifies the amounts of shifting. For example, if\
        \ \"direction\" is \"Right\", X is [1, 4],\n and S is [1, 1], the corresponding\
        \ output Z would be [0, 2]. If \"direction\" is \"LEFT\" with\n X=[1, 2] and\
        \ S=[1, 2], the corresponding output Y would be [2, 8].\n\n Because this operator\
        \ supports Numpy-style broadcasting, X's and Y's shapes are\n not necessarily\
        \ identical.\nThis operator supports **multidirectional (i.e., Numpy-style)\
        \ broadcasting**; for more details please check [the doc](Broadcasting.md)."
    arguments:
    - X
    - Y
    expression_string: onnx_ops.bitshift(X, Y, direction)
onnx::Cast:
    description: "\nThe operator casts the elements of a given input tensor to a data\
        \ type\nspecified by the 'to' argument and returns an output tensor of the\
        \ same size in\nthe converted type. The 'to' argument must be one of the data\
        \ types specified\nin the 'DataType' enum field in the TensorProto message.\n\
        \nCasting from string tensor in plain (e.g., \"3.14\" and \"1000\") and scientific\
        \ numeric representations\n(e.g., \"1e-5\" and \"1E8\") to float types is\
        \ supported. For example, converting string \"100.5\" to an integer may\n\
        result 100. There are some string literals reserved for special floating-point\
        \ values;\n\"+INF\" (and \"INF\"), \"-INF\", and \"NaN\" are positive infinity,\
        \ negative infinity, and not-a-number, respectively.\nAny string which can\
        \ exactly match \"+INF\" in a case-insensitive way would be mapped to positive\
        \ infinite. Similarly,\nthis case-insensitive rule is applied to \"INF\" and\
        \ \"NaN\". When casting from numeric tensors\nto string tensors, plain floating-point\
        \ representation (such as \"314.15926\") would be used.\nConverting non-numerical-literal\
        \ string such as \"Hello World!\" is an undefined behavior. Cases\nof converting\
        \ string representing floating-point arithmetic value, such as \"2.718\",\
        \ to INT is an undefined behavior.\n\nConversion from a numerical type to\
        \ any numerical type is always allowed.\nUser must be aware of precision loss\
        \ and value change caused by range difference between two types.\nFor example,\
        \ a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly,\
        \ converting\nan integer 36 to Boolean may produce 1 because we truncate bits\
        \ which can't be stored in the targeted type.\n\nIn more detail, the conversion\
        \ among numerical types should follow these rules:\n\n* Casting from floating\
        \ point to:\n  * floating point: +/- infinity if OOR (out of range).\n  *\
        \ fixed point: undefined if OOR.\n  * bool: +/- 0.0 to False; all else to\
        \ True.\n* Casting from fixed point to:\n  * floating point: +/- infinity\
        \ if OOR. (+ infinity in the case of uint)\n  * fixed point: when OOR, discard\
        \ higher bits and reinterpret (with respect to two's complement representation\
        \ for\nsigned types). For example, 200 (int16) -> -56 (int8).\n  * bool: zero\
        \ to False; nonzero to True.\n* Casting from bool to:\n  * floating point:\
        \ `{1.0, 0.0}`.\n  * fixed point: `{1, 0}`.\n  * bool: no change.\n"
    arguments:
    - input
    expression_string: onnx_ops.cast(input, to)
onnx::CastLike:
    description: '

        The operator casts the elements of a given input tensor (the first input)
        to

        the same data type as the elements of the second input tensor.

        See documentation of the Cast operator for further details.

        '
    arguments:
    - input
    - target_type
    expression_string: onnx_ops.castlike(input, target_type)
onnx::Ceil:
    description: '

        Ceil takes one input data (Tensor<T>) and produces one output data

        (Tensor<T>) where the ceil is, y = ceil(x), is applied to

        the tensor elementwise.

        '
    arguments:
    - X
    expression_string: onnx_ops.ceil(X)
onnx::Celu:
    description: '

        Continuously Differentiable Exponential Linear Units:

        Perform the linear unit element-wise on the input tensor X

        using formula:


        ```

        max(0,x) + min(0,alpha*(exp(x/alpha)-1))

        ```

        '
    arguments:
    - X
    expression_string: onnx_ops.celu(X, alpha)
onnx::Clip:
    description: '

        Clip operator limits the given input within an interval. The interval is

        specified by the inputs ''min'' and ''max''. They default to

        numeric_limits::lowest() and numeric_limits::max(), respectively.

        '
    arguments:
    - input
    - min
    - max
    expression_string: onnx_ops.clip(input, min, max)
onnx::Compress:
    description: "\n    Selects slices from an input tensor along a given axis where\
        \ condition evaluates to True for each axis index.\n    In case axis is not\
        \ provided, input is flattened before elements are selected.\n    Compress\
        \ behaves like numpy.compress: https://docs.scipy.org/doc/numpy/reference/generated/numpy.compress.html\n\
        \    "
    arguments:
    - input
    - condition
    expression_string: onnx_ops.compress(input, condition, axis)
onnx::Concat:
    description: Concatenate a list of tensors into a single tensor. All input tensors
        must have the same shape, except for the dimension size of the axis to concatenate
        on.
    arguments:
    - inputs
    expression_string: onnx_ops.concat(inputs, axis)
onnx::ConcatFromSequence:
    description: '

        Concatenate a sequence of tensors into a single tensor.

        All input tensors must have the same shape, except for the dimension size
        of the axis to concatenate on.

        By default ''new_axis'' is 0, the behavior is similar to numpy.concatenate.

        When ''new_axis'' is 1, the behavior is similar to numpy.stack.

        '
    arguments:
    - input_sequence
    expression_string: onnx_ops.concatfromsequence(input_sequence, axis, new_axis)
onnx::Constant:
    description: '

        This operator produces a constant tensor. Exactly one of the provided attributes,
        either value, sparse_value,

        or value_* must be specified.

        '
    arguments: []
    expression_string: onnx_ops.constant(sparse_value, value, value_float, value_floats,
        value_int, value_ints, value_string, value_strings)
onnx::ConstantOfShape:
    description: '

        Generate a tensor with given value and shape.

        '
    arguments:
    - input
    expression_string: onnx_ops.constantofshape(input, value)
onnx::Conv:
    description: '

        The convolution operator consumes an input tensor and a filter, and

        computes the output.'
    arguments:
    - X
    - W
    - B
    expression_string: onnx_ops.conv(X, W, B, auto_pad, dilations, group, kernel_shape,
        pads, strides)
onnx::ConvInteger:
    description: '

        The integer convolution operator consumes an input tensor, its zero-point,
        a filter, and its zero-point,

        and computes the output. The production MUST never overflow. The accumulation
        may overflow if and only if in 32 bits.

        '
    arguments:
    - x
    - w
    - x_zero_point
    - w_zero_point
    expression_string: onnx_ops.convinteger(x, w, x_zero_point, w_zero_point, auto_pad,
        dilations, group, kernel_shape, pads, strides)
onnx::ConvTranspose:
    description: "\nThe convolution transpose operator consumes an input tensor and\
        \ a filter,\nand computes the output.\n\nIf the pads parameter is provided\
        \ the shape of the output is calculated via the following equation:\n\n  output_shape[i]\
        \ = stride[i] * (input_size[i] - 1) + output_padding[i] + ((kernel_shape[i]\
        \ - 1) * dilations[i] + 1) - pads[start_i] - pads[end_i]\n\noutput_shape can\
        \ also be explicitly specified in which case pads values are auto generated\
        \ using these equations:\n\n  total_padding[i] = stride[i] * (input_size[i]\
        \ - 1) + output_padding[i] + ((kernel_shape[i] - 1) * dilations[i] + 1) -\
        \ output_shape[i]\n  If (auto_pads == SAME_UPPER): pads[start_i] = total_padding[i]/2;\
        \ pads[end_i] = total_padding[i] - (total_padding[i]/2)\n  Else: pads[start_i]\
        \ = total_padding[i] - (total_padding[i]/2); pads[end_i] = (total_padding[i]/2).\n\
        \n    "
    arguments:
    - X
    - W
    - B
    expression_string: onnx_ops.convtranspose(X, W, B, auto_pad, dilations, group,
        kernel_shape, output_padding, output_shape, pads, strides)
onnx::Cos:
    description: '

        Calculates the cosine of the given input tensor, element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.cos(input)
onnx::Cosh:
    description: '

        Calculates the hyperbolic cosine of the given input tensor element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.cosh(input)
onnx::CumSum:
    description: "\nPerforms cumulative sum of the input elements along the given\
        \ axis.\nBy default, it will do the sum inclusively meaning the first element\
        \ is copied as is.\nThrough an `exclusive` attribute, this behavior can change\
        \ to exclude the first element.\nIt can also perform summation in the opposite\
        \ direction of the axis. For that, set `reverse` attribute to 1.\n\nExample:\n\
        ```\ninput_x = [1, 2, 3]\naxis=0\noutput = [1, 3, 6]\nexclusive=1\noutput\
        \ = [0, 1, 3]\nexclusive=0\nreverse=1\noutput = [6, 5, 3]\nexclusive=1\nreverse=1\n\
        output = [5, 3, 0]\n```\n "
    arguments:
    - x
    - axis
    expression_string: onnx_ops.cumsum(x, axis, exclusive, reverse)
onnx::DepthToSpace:
    description: 'DepthToSpace rearranges (permutes) data from depth into blocks of
        spatial data.

        This is the reverse transformation of SpaceToDepth. More specifically, this
        op outputs a copy of

        the input tensor where values from the depth dimension are moved in spatial
        blocks to the height

        and width dimensions. By default, `mode` = `DCR`.

        In the DCR mode, elements along the depth dimension from the input tensor
        are rearranged in the

        following order: depth, column, and then row. The output y is computed from
        the input x as below:


        b, c, h, w = x.shape


        tmp = np.reshape(x, [b, blocksize, blocksize, c // (blocksize**2), h, w])


        tmp = np.transpose(tmp, [0, 3, 4, 1, 5, 2])


        y = np.reshape(tmp, [b, c // (blocksize**2), h * blocksize, w * blocksize])



        In the CRD mode, elements along the depth dimension from the input tensor
        are rearranged in the

        following order: column, row, and the depth. The output y is computed from
        the input x as below:


        b, c, h, w = x.shape


        tmp = np.reshape(x, [b, c // (blocksize ** 2), blocksize, blocksize, h, w])


        tmp = np.transpose(tmp, [0, 1, 4, 2, 5, 3])


        y = np.reshape(tmp, [b, c // (blocksize ** 2), h * blocksize, w * blocksize])


        '
    arguments:
    - input
    expression_string: onnx_ops.depthtospace(input, blocksize, mode)
onnx::DequantizeLinear:
    description: '

        The linear dequantization operator. It consumes a quantized tensor, a scale,
        and a zero point to compute the full precision tensor.

        The dequantization formula is y = (x - x_zero_point) * x_scale. ''x_scale''
        and ''x_zero_point'' must have same shape, and can be either a scalar

        for per-tensor / per layer quantization, or a 1-D tensor for per-axis quantization.

        ''x_zero_point'' and ''x'' must have same type. ''x'' and ''y'' must have
        same shape. In the case of dequantizing int32,

        there''s no zero point (zero point is supposed to be 0).

        '
    arguments:
    - x
    - x_scale
    - x_zero_point
    expression_string: onnx_ops.dequantizelinear(x, x_scale, x_zero_point, axis)
onnx::Det:
    description: '

        Det calculates determinant of a square matrix or batches of square matrices.

        Det takes one input tensor of shape `[*, M, M]`, where `*` is zero or more
        batch dimensions,

        and the inner-most 2 dimensions form square matrices.

        The output is a tensor of shape `[*]`, containing the determinants of all
        input submatrices.

        e.g., When the input is 2-D, the output is a scalar(shape is empty: `[]`).

        '
    arguments:
    - X
    expression_string: onnx_ops.det(X)
onnx::Div:
    description: '

        Performs element-wise binary division (with Numpy-style broadcasting support).


        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).


        (Opset 14 change): Extend supported types to include uint8, int8, uint16,
        and int16.

        '
    arguments:
    - A
    - B
    expression_string: onnx_ops.div(A, B)
onnx::Dropout:
    description: '

        Dropout takes an input floating-point tensor, an optional input ratio (floating-point
        scalar) and an optional input training_mode (boolean scalar). It produces
        two tensor outputs,

        output (floating-point tensor) and mask (optional `Tensor<bool>`). If `training_mode`
        is true then the output Y will be a random dropout;

        Note that this Dropout scales the masked input data by the following equation,
        so to convert the trained model into inference mode,

        the user can simply not pass `training_mode` input or set it to false.

        ```

        output = scale * data * mask,

        ```

        where

        ```

        scale = 1. / (1. - ratio).

        ```

        This operator has **optional** inputs/outputs. See [the doc](IR.md) for more
        details about the representation of optional arguments. An empty string may
        be used in the place of an actual argument''s name to indicate a missing argument.
        Trailing optional arguments (those not followed by an argument that is present)
        may also be simply omitted.

        '
    arguments:
    - data
    - ratio
    - training_mode
    expression_string: onnx_ops.dropout(data, ratio, training_mode, seed)
onnx::DynamicQuantizeLinear:
    description: "\nA Function to fuse calculation for Scale, Zero Point and FP32->8Bit\
        \ convertion of FP32 Input data.\nOutputs Scale, ZeroPoint and Quantized Input\
        \ for a given FP32 Input.\nScale is calculated as:\n```\n y_scale = (max(x)\
        \ - min(x))/(qmax - qmin)\n * where qmax and qmin are max and min values for\
        \ quantization range .i.e [0, 255] in case of uint8\n * data range is adjusted\
        \ to include 0.\n```\nZero point is calculated as:\n```\nintermediate_zero_point\
        \ = qmin - min(x)/y_scale\ny_zero_point = cast(round(saturate(itermediate_zero_point)))\n\
        * where qmax and qmin are max and min values for quantization range .i.e [0,\
        \ 255] in case of uint8\n* for saturation, it saturates to [0, 255] if it's\
        \ uint8, or [-127, 127] if it's int8. Right now only uint8 is supported.\n\
        * rounding to nearest ties to even.\n```\nData quantization formula is:\n\
        ```\ny = saturate (round (x / y_scale) + y_zero_point)\n* for saturation,\
        \ it saturates to [0, 255] if it's uint8, or [-127, 127] if it's int8. Right\
        \ now only uint8 is supported.\n* rounding to nearest ties to even.\n```\n"
    arguments:
    - x
    expression_string: onnx_ops.dynamicquantizelinear(x)
onnx::Einsum:
    description: '

        An einsum of the form ```term1, term2 -> output-term``` produces an output
        tensor using the following equation


        ```output[output-term] = reduce-sum( input1[term1] * input2[term] )```


        where the reduce-sum performs a summation over all the indices occurring in
        the input terms (term1, term2)

        that do not occur in the output-term.


        The Einsum operator evaluates algebraic tensor operations on a sequence of
        tensors, using the Einstein summation

        convention. The equation string contains a comma-separated sequence of lower
        case letters. Each term corresponds to

        an operand tensor, and the characters within the terms correspond to operands
        dimensions.


        This sequence may be followed by "->" to separate the left and right hand
        side of the equation.

        If the equation contains "->" followed by the right-hand side, the explicit
        (not classical) form of the Einstein

        summation is performed, and the right-hand side indices indicate output tensor
        dimensions. In other cases,

        output indices are (implicitly) set to the alphabetically sorted sequence
        of indices appearing exactly once in the

        equation.


        When a dimension character is repeated in the left-hand side, it represents
        summation along the dimension.


        The equation may contain ellipsis ("...") to enable broadcasting. Ellipsis
        must indicate a fixed number of dimensions.

        Specifically, every occurrence of ellipsis in the equation must represent
        the same number of dimensions.

        The right-hand side may contain exactly one ellipsis. In implicit mode, the
        ellipsis dimensions are set to the

        beginning of the output. The equation string may contain space (U+0020) character.

        '
    arguments:
    - Inputs
    expression_string: onnx_ops.einsum(Inputs, equation)
onnx::Elu:
    description: '

        Elu takes one input data (Tensor<T>) and produces one output data

        (Tensor<T>) where the function `f(x) = alpha * (exp(x) - 1.) for x <

        0`, `f(x) = x for x >= 0`., is applied to the tensor elementwise.


        '
    arguments:
    - X
    expression_string: onnx_ops.elu(X, alpha)
onnx::Equal:
    description: '

        Returns the tensor resulted from performing the `equal` logical operation

        elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting
        support).


        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).

        '
    arguments:
    - A
    - B
    expression_string: onnx_ops.equal(A, B)
onnx::Erf:
    description: '

        Computes the error function of the given input tensor element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.erf(input)
onnx::Exp:
    description: '

        Calculates the exponential of the given input tensor, element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.exp(input)
onnx::Expand:
    description: '

        Broadcast the input tensor following the given shape and the broadcast rule.

        The broadcast rule is similar to numpy.array(input) * numpy.ones(shape):

        Dimensions are right alignment;

        Two corresponding dimensions must have the same value, or one of them is equal
        to 1.

        Also, this operator is similar to numpy.broadcast_to(input, shape),

        but the major difference is numpy.broadcast_to() does not allow shape to be
        smaller than input.size().

        It is possible that the output.shape is not equal to shape, when some dimensions
        in shape is equal to 1,

        or the shape.ndim < input.shape.ndim.

        '
    arguments:
    - input
    - shape
    expression_string: onnx_ops.expand(input, shape)
onnx::EyeLike:
    description: '

        Generate a 2D tensor (matrix) with ones on the diagonal and zeros everywhere
        else. Only 2D

        tensors are supported, i.e. input T1 must be of rank 2. The shape of the output
        tensor is the

        same as the input tensor. The data type can be specified by the ''dtype''
        argument. If

        ''dtype'' is not specified, then the type of input tensor is used. By default,
        the main diagonal

        is populated with ones, but attribute ''k'' can be used to populate upper
        or lower diagonals.

        The ''dtype'' argument must be one of the data types specified in the ''DataType''
        enum field in the

        TensorProto message and be valid as an output type.

        '
    arguments:
    - input
    expression_string: onnx_ops.eyelike(input, dtype, k)
onnx::Flatten:
    description: '

        Flattens the input tensor into a 2D matrix. If input tensor has shape

        (d_0, d_1, ... d_n) then the output will have shape

        (d_0 X d_1 ... d_(axis-1), d_axis X d_(axis+1) ... X dn).

        '
    arguments:
    - input
    expression_string: onnx_ops.flatten(input, axis)
onnx::Floor:
    description: '

        Floor takes one input data (Tensor<T>) and produces one output data

        (Tensor<T>) where the floor is, y = floor(x), is applied to

        the tensor elementwise.

        '
    arguments:
    - X
    expression_string: onnx_ops.floor(X)
onnx::GRU:
    description: "\nComputes an one-layer GRU. This operator is usually supported\
        \ via some custom\nimplementation such as CuDNN.\n\nNotations:\n\n`X` - input\
        \ tensor\n\n`z` - update gate\n\n`r` - reset gate\n\n`h` - hidden gate\n\n\
        `t` - time step (t-1 means previous time step)\n\n`W[zrh]` - W parameter weight\
        \ matrix for update, reset, and hidden gates\n\n`R[zrh]` - R recurrence weight\
        \ matrix for update, reset, and hidden gates\n\n`Wb[zrh]` - W bias vectors\
        \ for update, reset, and hidden gates\n\n`Rb[zrh]` - R bias vectors for update,\
        \ reset, and hidden gates\n\n`WB[zrh]` - W parameter weight matrix for backward\
        \ update, reset, and hidden gates\n\n`RB[zrh]` - R recurrence weight matrix\
        \ for backward update, reset, and hidden gates\n\n`WBb[zrh]` - W bias vectors\
        \ for backward update, reset, and hidden gates\n\n`RBb[zrh]` - R bias vectors\
        \ for backward update, reset, and hidden gates\n\n`H` - Hidden state\n\n`num_directions`\
        \ - 2 if direction == bidirectional else 1\n\nActivation functions:\n\n  Relu(x)\
        \                - max(0, x)\n\n  Tanh(x)                - (1 - e^{-2x})/(1\
        \ + e^{-2x})\n\n  Sigmoid(x)             - 1/(1 + e^{-x})\n\n  (NOTE: Below\
        \ are optional)\n\n  Affine(x)              - alpha*x + beta\n\n  LeakyRelu(x)\
        \           - x if x >= 0 else alpha * x\n\n  ThresholdedRelu(x)     - x if\
        \ x >= alpha else 0\n\n  ScaledTanh(x)          - alpha*Tanh(beta*x)\n\n \
        \ HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)\n\n  Elu(x)    \
        \             - x if x >= 0 else alpha*(e^x - 1)\n\n  Softsign(x)        \
        \    - x/(1 + |x|)\n\n  Softplus(x)            - log(1 + e^x)\n\nEquations\
        \ (Default: f=Sigmoid, g=Tanh):\n\n  - zt = f(Xt*(Wz^T) + Ht-1*(Rz^T) + Wbz\
        \ + Rbz)\n\n  - rt = f(Xt*(Wr^T) + Ht-1*(Rr^T) + Wbr + Rbr)\n\n  - ht = g(Xt*(Wh^T)\
        \ + (rt (.) Ht-1)*(Rh^T) + Rbh + Wbh) # default, when linear_before_reset\
        \ = 0\n\n  - ht = g(Xt*(Wh^T) + (rt (.) (Ht-1*(Rh^T) + Rbh)) + Wbh) # when\
        \ linear_before_reset != 0\n\n  - Ht = (1 - zt) (.) ht + zt (.) Ht-1\nThis\
        \ operator has **optional** inputs/outputs. See [the doc](IR.md) for more\
        \ details about the representation of optional arguments. An empty string\
        \ may be used in the place of an actual argument's name to indicate a missing\
        \ argument. Trailing optional arguments (those not followed by an argument\
        \ that is present) may also be simply omitted.\n"
    arguments:
    - X
    - W
    - R
    - B
    - sequence_lens
    - initial_h
    expression_string: onnx_ops.gru(X, W, R, B, sequence_lens, initial_h, activation_alpha,
        activation_beta, activations, clip, direction, hidden_size, layout, linear_before_reset)
onnx::Gather:
    description: "\nGiven `data` tensor of rank r >= 1, and `indices` tensor of rank\
        \ q, gather\nentries of the axis dimension of `data` (by default outer-most\
        \ one as axis=0) indexed by `indices`, and concatenates\nthem in an output\
        \ tensor of rank q + (r - 1).\n\naxis = 0 :\n\nLet\nk = indices[i_{0}, ...,\
        \ i_{q-1}]\nThen\noutput[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k\
        \ , j_{0}, ..., j_{r-2}]\n\n```\n  data = [\n      [1.0, 1.2],\n      [2.3,\
        \ 3.4],\n      [4.5, 5.7],\n  ]\n  indices = [\n      [0, 1],\n      [1, 2],\n\
        \  ]\n  output = [\n      [\n          [1.0, 1.2],\n          [2.3, 3.4],\n\
        \      ],\n      [\n          [2.3, 3.4],\n          [4.5, 5.7],\n      ],\n\
        \  ]\n```\naxis = 1 :\n\nLet\nk = indices[i_{0}, ..., i_{q-1}]\nThen\noutput[j_{0},\
        \ i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2}] = input[j_{0}, k, j_{1}, ...,\
        \ j_{r-2}]\n\n```\n  data = [\n      [1.0, 1.2, 1.9],\n      [2.3, 3.4, 3.9],\n\
        \      [4.5, 5.7, 5.9],\n  ]\n  indices = [\n      [0, 2],\n  ]\n  axis =\
        \ 1,\n  output = [\n          [[1.0, 1.9]],\n          [[2.3, 3.9]],\n   \
        \       [[4.5, 5.9]],\n  ]\n```\n"
    arguments:
    - data
    - indices
    expression_string: onnx_ops.gather(data, indices, axis)
onnx::GatherElements:
    description: "\n\nGatherElements takes two inputs `data` and `indices` of the\
        \ same rank r >= 1\nand an optional attribute `axis` that identifies an axis\
        \ of `data`\n(by default, the outer-most axis, that is axis 0). It is an indexing\
        \ operation\nthat produces its output by indexing into the input data tensor\
        \ at index\npositions determined by elements of the `indices` tensor.\nIts\
        \ output shape is the same as the shape of `indices` and consists of one value\n\
        (gathered from the `data`) for each element in `indices`.\n\nFor instance,\
        \ in the 3-D case (r = 3), the output produced is determined\nby the following\
        \ equations:\n```\n  out[i][j][k] = input[index[i][j][k]][j][k] if axis =\
        \ 0,\n  out[i][j][k] = input[i][index[i][j][k]][k] if axis = 1,\n  out[i][j][k]\
        \ = input[i][j][index[i][j][k]] if axis = 2,\n```\n\nThis operator is also\
        \ the inverse of ScatterElements. It is similar to Torch's gather operation.\n\
        \nExample 1:\n```\n  data = [\n      [1, 2],\n      [3, 4],\n  ]\n  indices\
        \ = [\n      [0, 0],\n      [1, 0],\n  ]\n  axis = 1\n  output = [\n     \
        \ [1, 1],\n      [4, 3],\n  ]\n```\nExample 2:\n```\n  data = [\n      [1,\
        \ 2, 3],\n      [4, 5, 6],\n      [7, 8, 9],\n  ]\n  indices = [\n      [1,\
        \ 2, 0],\n      [2, 0, 0],\n  ]\n  axis = 0\n  output = [\n      [4, 8, 3],\n\
        \      [7, 2, 3],\n  ]\n```\n"
    arguments:
    - data
    - indices
    expression_string: onnx_ops.gatherelements(data, indices, axis)
onnx::GatherND:
    description: "\nGiven `data` tensor of rank `r` >= 1, `indices` tensor of rank\
        \ `q` >= 1, and `batch_dims` integer `b`, this operator gathers\nslices of\
        \ `data` into an output tensor of rank `q + r - indices_shape[-1] - 1 - b`.\n\
        \n`indices` is an q-dimensional integer tensor, best thought of as a `(q-1)`-dimensional\
        \ tensor of index-tuples into `data`,\nwhere each element defines a slice\
        \ of `data`\n\n`batch_dims` (denoted as `b`) is an integer indicating the\
        \ number of batch dimensions, i.e the leading `b` number of dimensions of\n\
        `data` tensor and `indices` are representing the batches, and the gather starts\
        \ from the `b+1` dimension.\n\nSome salient points about the inputs' rank\
        \ and shape:\n\n1) r >= 1 and q >= 1 are to be honored. There is no dependency\
        \ condition to be met between ranks `r` and `q`\n\n2) The first `b` dimensions\
        \ of the shape of `indices` tensor and `data` tensor must be equal.\n\n3)\
        \ b < min(q, r) is to be honored.\n\n4) The `indices_shape[-1]` should have\
        \ a value between 1 (inclusive) and rank `r-b` (inclusive)\n\n5) All values\
        \ in `indices` are expected to be within bounds [-s, s-1] along axis of size\
        \ `s` (i.e.) `-data_shape[i] <= indices[...,i] <= data_shape[i] - 1`.\n  \
        \ It is an error if any of the index values are out of bounds.\n\nThe output\
        \ is computed as follows:\n\nThe output tensor is obtained by mapping each\
        \ index-tuple in the `indices` tensor to the corresponding slice of the input\
        \ `data`.\n\n1) If `indices_shape[-1] > r-b` => error condition\n\n2) If `indices_shape[-1]\
        \ == r-b`, since the rank of `indices` is `q`, `indices` can be thought of\
        \ as `N` `(q-b-1)`-dimensional tensors\n   containing 1-D tensors of dimension\
        \ `r-b`, where `N` is an integer equals to the product of 1 and all the elements\
        \ in the batch dimensions\n   of the indices_shape. Let us think of each such\
        \ `r-b` ranked tensor as `indices_slice`. Each *scalar value* corresponding\
        \ to `data[0:b-1,indices_slice]`\n   is filled into the corresponding location\
        \ of the `(q-b-1)`-dimensional tensor to form the `output` tensor (Example\
        \ 1 below)\n\n3) If `indices_shape[-1] < r-b`, since the rank of `indices`\
        \ is `q`, `indices` can be thought of as `N` `(q-b-1)`-dimensional tensor\n\
        \   containing 1-D tensors of dimension `< r-b`. Let us think of each such\
        \ tensors as `indices_slice`. Each *tensor slice* corresponding\n   to `data[0:b-1,\
        \ indices_slice , :]` is filled into the corresponding location of the `(q-b-1)`-dimensional\
        \ tensor\n   to form the `output` tensor (Examples 2, 3, 4 and 5 below)\n\n\
        This operator is the inverse of `ScatterND`.\n\n`Example 1`\n\n  batch_dims\
        \ = 0\n\n  data    = [[0,1],[2,3]]   # data_shape = [2, 2]\n\n  indices =\
        \ [[0,0],[1,1]]   # indices_shape = [2, 2]\n\n  output  = [0,3]          \
        \ # output_shape = [2]\n\n`Example 2`\n\n  batch_dims = 0\n\n  data    = [[0,1],[2,3]]\
        \  # data_shape = [2, 2]\n\n  indices = [[1],[0]]      # indices_shape = [2,\
        \ 1]\n\n  output  = [[2,3],[0,1]]  # output_shape = [2, 2]\n\n`Example 3`\n\
        \n  batch_dims = 0\n\n  data    = [[[0,1],[2,3]],[[4,5],[6,7]]] # data_shape\
        \ = [2, 2, 2]\n\n  indices = [[0,1],[1,0]]                 # indices_shape\
        \ = [2, 2]\n\n  output  = [[2,3],[4,5]]                 # output_shape = [2,\
        \ 2]\n\n`Example 4`\n\n  batch_dims = 0\n\n  data    = [[[0,1],[2,3]],[[4,5],[6,7]]]\
        \ # data_shape = [2, 2, 2]\n\n  indices = [[[0,1]],[[1,0]]]             #\
        \ indices_shape = [2, 1, 2]\n\n  output  = [[[2,3]],[[4,5]]]             #\
        \ output_shape = [2, 1, 2]\n\n`Example 5`\n\n  batch_dims = 1\n\n  data  \
        \  = [[[0,1],[2,3]],[[4,5],[6,7]]] # data_shape = [2, 2, 2]\n\n  indices =\
        \ [[1],[0]]             # indices_shape = [2, 1]\n\n  output  = [[2,3],[4,5]]\
        \             # output_shape = [2, 2]\n\n\n"
    arguments:
    - data
    - indices
    expression_string: onnx_ops.gathernd(data, indices, batch_dims)
onnx::Gemm:
    description: 'General Matrix multiplication:

        https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3


        A'' = transpose(A) if transA else A


        B'' = transpose(B) if transB else B


        Compute Y = alpha * A'' * B'' + beta * C, where input tensor A has shape (M,
        K) or (K, M),

        input tensor B has shape (K, N) or (N, K), input tensor C is broadcastable
        to shape (M, N),

        and output tensor Y has shape (M, N). A will be transposed before doing the

        computation if attribute transA is non-zero, same for B and transB.

        This operator supports **unidirectional broadcasting** (tensor C should be
        unidirectional broadcastable to tensor A * B); for more details please check
        [the doc](Broadcasting.md).

        This operator has **optional** inputs/outputs. See [the doc](IR.md) for more
        details about the representation of optional arguments. An empty string may
        be used in the place of an actual argument''s name to indicate a missing argument.
        Trailing optional arguments (those not followed by an argument that is present)
        may also be simply omitted.

        '
    arguments:
    - A
    - B
    - C
    expression_string: onnx_ops.gemm(A, B, C, alpha, beta, transA, transB)
onnx::GlobalAveragePool:
    description: "\n GlobalAveragePool consumes an input tensor X and applies average\
        \ pooling across\n the values in the same channel. This is equivalent to AveragePool\
        \ with kernel size\n equal to the spatial dimension of input tensor."
    arguments:
    - X
    expression_string: onnx_ops.globalaveragepool(X)
onnx::GlobalLpPool:
    description: "\n GlobalLpPool consumes an input tensor X and applies lp pool pooling\
        \ across\n the values in the same channel. This is equivalent to LpPool with\
        \ kernel size\n equal to the spatial dimension of input tensor."
    arguments:
    - X
    expression_string: onnx_ops.globallppool(X, p)
onnx::GlobalMaxPool:
    description: "\n GlobalMaxPool consumes an input tensor X and applies max pooling\
        \ across\n the values in the same channel. This is equivalent to MaxPool with\
        \ kernel size\n equal to the spatial dimension of input tensor."
    arguments:
    - X
    expression_string: onnx_ops.globalmaxpool(X)
onnx::Greater:
    description: '

        Returns the tensor resulted from performing the `greater` logical operation

        elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting
        support).


        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).

        '
    arguments:
    - A
    - B
    expression_string: onnx_ops.greater(A, B)
onnx::GreaterOrEqual:
    description: '

        Returns the tensor resulted from performing the `greater_equal` logical operation

        elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting
        support).


        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).

        '
    arguments:
    - A
    - B
    expression_string: onnx_ops.greaterorequal(A, B)
onnx::HardSigmoid:
    description: '

        HardSigmoid takes one input data (Tensor<T>) and produces one output data

        (Tensor<T>) where the HardSigmoid function, y = max(0, min(1, alpha * x +
        beta)),

        is applied to the tensor elementwise.

        '
    arguments:
    - X
    expression_string: onnx_ops.hardsigmoid(X, alpha, beta)
onnx::HardSwish:
    description: '

        HardSwish takes one input data (Tensor<T>) and produces one output data (Tensor<T>)
        where

        the HardSwish function, y = x * max(0, min(1, alpha * x + beta)) = x * HardSigmoid<alpha,
        beta>(x),

        where alpha = 1/6 and beta = 0.5, is applied to the tensor elementwise.

        '
    arguments:
    - X
    expression_string: onnx_ops.hardswish(X)
onnx::Hardmax:
    description: "\nThe operator computes the hardmax values for the given input:\n\
        \n Hardmax(element in input, axis) = 1 if the element is the first maximum\
        \ value along the specified axis, 0 otherwise\n\nThe \"axis\" attribute indicates\
        \ the dimension along which Hardmax\nwill be performed. The output tensor\
        \ has the same shape\nand contains the Hardmax values of the corresponding\
        \ input.\n"
    arguments:
    - input
    expression_string: onnx_ops.hardmax(input, axis)
onnx::Identity:
    description: Identity operator
    arguments:
    - input
    expression_string: onnx_ops.identity(input)
onnx::InstanceNormalization:
    description: '

        Carries out instance normalization as described in the paper

        https://arxiv.org/abs/1607.08022.


        y = scale * (x - mean) / sqrt(variance + epsilon) + B,

        where mean and variance are computed per instance per channel.


        '
    arguments:
    - input
    - scale
    - B
    expression_string: onnx_ops.instancenormalization(input, scale, B, epsilon)
onnx::IsInf:
    description: Map infinity to true and other values to false.
    arguments:
    - X
    expression_string: onnx_ops.isinf(X, detect_negative, detect_positive)
onnx::IsNaN:
    description: Returns which elements of the input are NaN.
    arguments:
    - X
    expression_string: onnx_ops.isnan(X)
onnx::LRN:
    description: '

        Local Response Normalization proposed in the [AlexNet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).

        It normalizes over local input regions.

        The local region is defined across the channels. For an element X[n, c, d1,
        ..., dk] in a tensor

        of shape (N x C x D1 x D2, ..., Dk), its region is

        {X[n, i, d1, ..., dk] | max(0, c - floor((size - 1) / 2)) <= i <= min(C -
        1, c + ceil((size - 1) / 2))}.


        square_sum[n, c, d1, ..., dk] = sum(X[n, i, d1, ..., dk] ^ 2),

        where max(0, c - floor((size - 1) / 2)) <= i <= min(C - 1, c + ceil((size
        - 1) / 2)).


        Y[n, c, d1, ..., dk] = X[n, c, d1, ..., dk] / (bias + alpha / size * square_sum[n,
        c, d1, ..., dk] ) ^ beta

        '
    arguments:
    - X
    expression_string: onnx_ops.lrn(X, alpha, beta, bias, size)
onnx::LSTM:
    description: "\nComputes an one-layer LSTM. This operator is usually supported\
        \ via some\ncustom implementation such as CuDNN.\n\nNotations:\n\n`X` - input\
        \ tensor\n\n`i` - input gate\n\n`o` - output gate\n\n`f` - forget gate\n\n\
        `c` - cell gate\n\n`t` - time step (t-1 means previous time step)\n\n`W[iofc]`\
        \ - W parameter weight matrix for input, output, forget, and cell gates\n\n\
        `R[iofc]` - R recurrence weight matrix for input, output, forget, and cell\
        \ gates\n\n`Wb[iofc]` - W bias vectors for input, output, forget, and cell\
        \ gates\n\n`Rb[iofc]` - R bias vectors for input, output, forget, and cell\
        \ gates\n\n`P[iof]`  - P peephole weight vector for input, output, and forget\
        \ gates\n\n`WB[iofc]` - W parameter weight matrix for backward input, output,\
        \ forget, and cell gates\n\n`RB[iofc]` - R recurrence weight matrix for backward\
        \ input, output, forget, and cell gates\n\n`WBb[iofc]` - W bias vectors for\
        \ backward input, output, forget, and cell gates\n\n`RBb[iofc]` - R bias vectors\
        \ for backward input, output, forget, and cell gates\n\n`PB[iof]`  - P peephole\
        \ weight vector for backward input, output, and forget gates\n\n`H` - Hidden\
        \ state\n\n`num_directions` - 2 if direction == bidirectional else 1\n\nActivation\
        \ functions:\n\n  Relu(x)                - max(0, x)\n\n  Tanh(x)        \
        \        - (1 - e^{-2x})/(1 + e^{-2x})\n\n  Sigmoid(x)             - 1/(1\
        \ + e^{-x})\n\n  (NOTE: Below are optional)\n\n  Affine(x)              -\
        \ alpha*x + beta\n\n  LeakyRelu(x)           - x if x >= 0 else alpha * x\n\
        \n  ThresholdedRelu(x)     - x if x >= alpha else 0\n\n  ScaledTanh(x)   \
        \       - alpha*Tanh(beta*x)\n\n  HardSigmoid(x)         - min(max(alpha*x\
        \ + beta, 0), 1)\n\n  Elu(x)                 - x if x >= 0 else alpha*(e^x\
        \ - 1)\n\n  Softsign(x)            - x/(1 + |x|)\n\n  Softplus(x)        \
        \    - log(1 + e^x)\n\nEquations (Default: f=Sigmoid, g=Tanh, h=Tanh):\n\n\
        \  - it = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Pi (.) Ct-1 + Wbi + Rbi)\n\n  - ft =\
        \ f(Xt*(Wf^T) + Ht-1*(Rf^T) + Pf (.) Ct-1 + Wbf + Rbf)\n\n  - ct = g(Xt*(Wc^T)\
        \ + Ht-1*(Rc^T) + Wbc + Rbc)\n\n  - Ct = ft (.) Ct-1 + it (.) ct\n\n  - ot\
        \ = f(Xt*(Wo^T) + Ht-1*(Ro^T) + Po (.) Ct + Wbo + Rbo)\n\n  - Ht = ot (.)\
        \ h(Ct)\nThis operator has **optional** inputs/outputs. See [the doc](IR.md)\
        \ for more details about the representation of optional arguments. An empty\
        \ string may be used in the place of an actual argument's name to indicate\
        \ a missing argument. Trailing optional arguments (those not followed by an\
        \ argument that is present) may also be simply omitted.\n"
    arguments:
    - X
    - W
    - R
    - B
    - sequence_lens
    - initial_h
    - initial_c
    - P
    expression_string: onnx_ops.lstm(X, W, R, B, sequence_lens, initial_h, initial_c,
        P, activation_alpha, activation_beta, activations, clip, direction, hidden_size,
        input_forget, layout)
onnx::LeakyRelu:
    description: '

        LeakyRelu takes input data (Tensor<T>) and an argument alpha, and produces
        one

        output data (Tensor<T>) where the function `f(x) = alpha * x for x < 0`,

        `f(x) = x for x >= 0`, is applied to the data tensor elementwise.

        '
    arguments:
    - X
    expression_string: onnx_ops.leakyrelu(X, alpha)
onnx::Less:
    description: '

        Returns the tensor resulted from performing the `less` logical operation

        elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting
        support).


        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).

        '
    arguments:
    - A
    - B
    expression_string: onnx_ops.less(A, B)
onnx::LessOrEqual:
    description: '

        Returns the tensor resulted from performing the `less_equal` logical operation

        elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting
        support).


        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).

        '
    arguments:
    - A
    - B
    expression_string: onnx_ops.lessorequal(A, B)
onnx::Log:
    description: '

        Calculates the natural log of the given input tensor, element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.log(input)
onnx::LogSoftmax:
    description: "\nThe operator computes the log of softmax values for the given\
        \ input:\n\n LogSoftmax(input, axis) = Log(Softmax(input, axis=axis))\n\n\
        The \"axis\" attribute indicates the dimension along which LogSoftmax\nwill\
        \ be performed. The output tensor has the same shape\nand contains the LogSoftmax\
        \ values of the corresponding input.\n"
    arguments:
    - input
    expression_string: onnx_ops.logsoftmax(input, axis)
onnx::LpNormalization:
    description: '

        Given a matrix, apply Lp-normalization along the provided axis.

        '
    arguments:
    - input
    expression_string: onnx_ops.lpnormalization(input, axis, p)
onnx::LpPool:
    description: "\n LpPool consumes an input tensor X and applies Lp pooling across\n\
        \ the tensor according to kernel sizes, stride sizes, and pad lengths.\n Lp\
        \ pooling consisting of computing the Lp norm on all values of a subset\n\
        \ of the input tensor according to the kernel size and downsampling the\n\
        \ data into the output tensor Y for further processing."
    arguments:
    - X
    expression_string: onnx_ops.lppool(X, auto_pad, kernel_shape, p, pads, strides)
onnx::MatMul:
    description: '

        Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html

        '
    arguments:
    - A
    - B
    expression_string: onnx_ops.matmul(A, B)
onnx::MatMulInteger:
    description: '

        Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html.

        The production MUST never overflow. The accumulation may overflow if and only
        if in 32 bits.

        '
    arguments:
    - A
    - B
    - a_zero_point
    - b_zero_point
    expression_string: onnx_ops.matmulinteger(A, B, a_zero_point, b_zero_point)
onnx::Max:
    description: '

        Element-wise max of each of the input tensors (with Numpy-style broadcasting
        support).

        All inputs and outputs must have the same data type.

        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).

        '
    arguments:
    - data_0
    expression_string: onnx_ops.max(data_0)
onnx::MaxPool:
    description: "\n MaxPool consumes an input tensor X and applies max pooling across\n\
        \ the tensor according to kernel sizes, stride sizes, and pad lengths.\n max\
        \ pooling consisting of computing the max on all values of a\n subset of the\
        \ input tensor according to the kernel size and downsampling the\n data into\
        \ the output tensor Y for further processing. The output spatial shape will\
        \ be following:\n ```\n output_spatial_shape[i] = floor((input_spatial_shape[i]\
        \ + pad_shape[i] - ((kernel_spatial_shape[i] - 1) * dilations[i] + 1)) / strides_spatial_shape[i]\
        \ + 1)\n ```\n or\n ```\n output_spatial_shape[i] = ceil((input_spatial_shape[i]\
        \ + pad_shape[i] - ((kernel_spatial_shape[i] - 1) * dilations[i] + 1)) / strides_spatial_shape[i]\
        \ + 1)\n ```\n if ceil_mode is enabled\n\n ```\n * pad_shape[i] is sum of\
        \ pads along axis i\n ```\n\n `auto_pad` is a DEPRECATED attribute. If you\
        \ are using them currently, the output spatial shape will be following:\n\
        \ ```\n VALID: output_spatial_shape[i] = ceil((input_spatial_shape[i] - ((kernel_spatial_shape[i]\
        \ - 1) * dilations[i] + 1) + 1) / strides_spatial_shape[i])\n SAME_UPPER or\
        \ SAME_LOWER: output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides_spatial_shape[i])\n\
        \ ```\n And pad shape will be following if `SAME_UPPER` or `SAME_LOWER`:\n\
        \ ```\n pad_shape[i] = (output_spatial_shape[i] - 1) * strides_spatial_shape[i]\
        \ + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n\
        \ ```\n The output of each pooling window is maximum number of elements exclude\
        \ pad. \n "
    arguments:
    - X
    expression_string: onnx_ops.maxpool(X, auto_pad, ceil_mode, dilations, kernel_shape,
        pads, storage_order, strides)
onnx::MaxRoiPool:
    description: "\n ROI max pool consumes an input tensor X and region of interests\
        \ (RoIs) to\n apply max pooling across each RoI, to produce output 4-D tensor\
        \ of shape\n (num_rois, channels, pooled_shape[0], pooled_shape[1])."
    arguments:
    - X
    - rois
    expression_string: onnx_ops.maxroipool(X, rois, pooled_shape, spatial_scale)
onnx::MaxUnpool:
    description: "\nMaxUnpool essentially computes the partial inverse of the MaxPool\
        \ op.\n The input information to this op is typically the output information\
        \ from a MaxPool op. The first\n input tensor X is the tensor that needs to\
        \ be unpooled, which is typically the pooled tensor (first output)\n from\
        \ MaxPool. The second input tensor, I, contains the indices to the (locally\
        \ maximal) elements corrsponding\n to the elements in the first input tensor\
        \ X. Input tensor I is typically the second output of the MaxPool op.\n The\
        \ third (optional) input is a tensor that specifies the output size of the\
        \ unpooling operation.\n\nMaxUnpool is intended to do 'partial' inverse of\
        \ the MaxPool op. 'Partial' because all the non-maximal\n values from the\
        \ original input to MaxPool are set to zero in the output of the MaxUnpool\
        \ op. Pooling\n the result of an unpooling operation should give back the\
        \ original input to the unpooling op.\n\nMaxUnpool can produce the same output\
        \ size for several input sizes, which makes unpooling op ambiguous.\n The\
        \ third input argument, output_size, is meant to disambiguate the op and produce\
        \ output tensor of\n known/predictable size.\n\nIn addition to the inputs,\
        \ MaxUnpool takes three attributes, namely kernel_shape, strides, and pads,\n\
        \ which define the exact unpooling op. The attributes typically have the same\
        \ values as the corrsponding\n pooling op that the unpooling op is trying\
        \ to invert.\n"
    arguments:
    - X
    - I
    - output_shape
    expression_string: onnx_ops.maxunpool(X, I, output_shape, kernel_shape, pads,
        strides)
onnx::Mean:
    description: '

        Element-wise mean of each of the input tensors (with Numpy-style broadcasting
        support).

        All inputs and outputs must have the same data type.

        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).

        '
    arguments:
    - data_0
    expression_string: onnx_ops.mean(data_0)
onnx::MeanVarianceNormalization:
    description: "\n      A MeanVarianceNormalization Function: Perform mean variance\
        \ normalization\n      on the input tensor X using formula: <br/> ``` (X-EX)/sqrt(E(X-EX)^2)\
        \ ```\n"
    arguments:
    - X
    expression_string: onnx_ops.meanvariancenormalization(X, axes)
onnx::Min:
    description: '

        Element-wise min of each of the input tensors (with Numpy-style broadcasting
        support).

        All inputs and outputs must have the same data type.

        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).

        '
    arguments:
    - data_0
    expression_string: onnx_ops.min(data_0)
onnx::Mod:
    description: "\n  Performs element-wise binary modulus (with Numpy-style broadcasting\
        \ support).\n    The sign of the remainder is the same as that of the Divisor.\n\
        \n    Mod operator can also behave like C fmod() or numpy.fmod. In this case,\
        \ the sign of the remainder however, will be the same as the Dividend\n  \
        \  (in contrast to integer mod). To force a behavior like numpy.fmod() an\
        \ 'fmod' Attribute is provided.\n    This attribute is set to 0 by default\
        \ causing the behavior to be like integer mod.\n    Setting this attribute\
        \ to 1 causes the remainder to be calculated similar to that of numpy.fmod().\n\
        \n    If the input type is floating point, then `fmod` attribute must be set\
        \ to 1.\n\n    In case of dividend being zero, the results will be platform\
        \ dependent.\n\n  This operator supports **multidirectional (i.e., Numpy-style)\
        \ broadcasting**; for more details please check [the doc](Broadcasting.md).\n"
    arguments:
    - A
    - B
    expression_string: onnx_ops.mod(A, B, fmod)
onnx::Mul:
    description: '

        Performs element-wise binary multiplication (with Numpy-style broadcasting
        support).


        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).


        (Opset 14 change): Extend supported types to include uint8, int8, uint16,
        and int16.

        '
    arguments:
    - A
    - B
    expression_string: onnx_ops.mul(A, B)
onnx::Multinomial:
    description: '

        Generate a tensor of samples from a multinomial distribution according to
        the probabilities

        of each of the possible outcomes.

        '
    arguments:
    - input
    expression_string: onnx_ops.multinomial(input, dtype, sample_size, seed)
onnx::Neg:
    description: '

        Neg takes one input data (Tensor<T>) and produces one output data

        (Tensor<T>) where each element flipped sign, y = -x, is applied to

        the tensor elementwise.

        '
    arguments:
    - X
    expression_string: onnx_ops.neg(X)
onnx::NegativeLogLikelihoodLoss:
    description: "\nA NegativeLogLikelihoodLoss operator computes (weighted) negative\
        \ log likelihood loss.\nIts \"input\" tensor has the shape of (N, C, d1, d2,\
        \ ..., dk) where k >= 0.\nThe \"input\" tensor contains log-probabilities\
        \ for input[n, :, d_1, d_2,..., d_k] being in a class of [0, C).\nThe operator's\
        \ \"target\" input tensor has the shape of (N, d1, d2, ..., dk). It encodes\
        \ class labels (one of C classes)\nor it may contain a special value (indicated\
        \ by an attribute ignore_index) for N x d1 x d2 x ... x dk samples.\nThe loss\
        \ value for input[n, :, d_1, d_2,...d_k] being classified as class c = target[n][d_1][d_2]...[d_k]\
        \ is computed as:\n\n    loss[n][d_1][d_2]...[d_k] = -input[n][c][d_1][d_2]...[d_k].\n\
        \nWhen an optional \"weight\" is provided, the sample loss is calculated as:\n\
        \n    loss[n][d_1][d_2]...[d_k] = -input[n][c][d_1][d_2]...[d_k] * weight[c].\n\
        \nloss is zero for the case when target-value equals ignore_index.\n\n   \
        \ loss[n][d_1][d_2]...[d_k] = 0, when target[n][d_1][d_2]...[d_k] = ignore_index\n\
        \nIf \"reduction\" attribute is set to \"none\", the operator's output will\
        \ be the above loss with shape (N, d1, d2, ..., dk).\nIf \"reduction\" attribute\
        \ is set to \"mean\" (the default attribute value), the output loss is (weight)\
        \ averaged:\n\n    mean(loss), if \"weight\" is not provided,\n\nor if weight\
        \ is provided,\n\n    sum(loss) / sum(weight[target[n][d_1][d_2]...[d_k]]]),\
        \ for all samples.\n\nIf \"reduction\" attribute is set to \"sum\", the output\
        \ is a scalar:\n    sum(loss).\n\nSee also https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss.\n\
        \nExample 1:\n\n    // negative log likelihood loss, \"none\" reduction\n\
        \    N, C, d1 = 2, 3, 2\n    input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],\n\
        \             [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]\n    target = [[2, 1], [0,\
        \ 2]]\n\n    loss = np.zeros((N, d1))\n    for n in range(N):\n        for\
        \ d_1 in range(d1):\n            c = target[n][d_1]\n            loss[n][d_1]\
        \ = -input[n][c][d_1]\n\n    // print(loss)\n    // [[-3. -2.]\n    //  [-0.\
        \ -2.]]\n\nExample 2:\n\n    // weighted negative log likelihood loss, sum\
        \ reduction\n    N, C, d1 = 2, 3, 2\n    input = [[[1.0, 2.0], [2.0, 2.0],\
        \ [3.0, 2.0]],\n            [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]\n    target\
        \ = [[2, 1], [0, 2]]\n    weight = [0.2, 0.3, 0.1]\n    loss = np.zeros((N,\
        \ d1))\n    for n in range(N):\n        for d_1 in range(d1):\n          \
        \  c = target[n][d_1]\n            loss[n][d_1] = -input[n][c][d_1] * weight[c]\n\
        \n    loss = np.sum(loss)\n    // print(loss)\n    // -1.1\n\nExample 3:\n\
        \n    // weighted negative log likelihood loss, mean reduction\n    N, C,\
        \ d1 = 2, 3, 2\n    input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],\n     \
        \       [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]\n    target = [[2, 1], [0, 2]]\n\
        \    weight = [0.2, 0.3, 0.1]\n    loss = np.zeros((N, d1))\n    weight_total\
        \ = 0\n    for n in range(N):\n        for d_1 in range(d1):\n           \
        \ c = target[n][d_1]\n            loss[n][d_1] = -input[n][c][d_1] * weight[c]\n\
        \            weight_total = weight_total + weight[c]\n\n    loss = np.sum(loss)\
        \ / weight_total\n    // print(loss)\n    // -1.57\n"
    arguments:
    - input
    - target
    - weight
    expression_string: onnx_ops.negativeloglikelihoodloss(input, target, weight, ignore_index,
        reduction)
onnx::NonMaxSuppression:
    description: '

        Filter out boxes that have high intersection-over-union (IOU) overlap with
        previously selected boxes.

        Bounding boxes with score less than score_threshold are removed. Bounding
        box format is indicated by attribute center_point_box.

        Note that this algorithm is agnostic to where the origin is in the coordinate
        system and more generally is invariant to

        orthogonal transformations and translations of the coordinate system; thus
        translating or reflections of the coordinate system

        result in the same boxes being selected by the algorithm.

        The selected_indices output is a set of integers indexing into the input collection
        of bounding boxes representing the selected boxes.

        The bounding box coordinates corresponding to the selected indices can then
        be obtained using the Gather or GatherND operation.

        '
    arguments:
    - boxes
    - scores
    - max_output_boxes_per_class
    - iou_threshold
    - score_threshold
    expression_string: onnx_ops.nonmaxsuppression(boxes, scores, max_output_boxes_per_class,
        iou_threshold, score_threshold, center_point_box)
onnx::NonZero:
    description: "\n    Returns the indices of the elements that are non-zero\n  \
        \  (in row-major order - by dimension).\n    NonZero behaves similar to numpy.nonzero:\n\
        \    https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html,\n\
        \    but for scalar input, NonZero produces output shape (0, N) instead of\
        \ (1, N), which is different from Numpy's behavior.\n"
    arguments:
    - X
    expression_string: onnx_ops.nonzero(X)
onnx::Not:
    description: '

        Returns the negation of the input tensor element-wise.

        '
    arguments:
    - X
    expression_string: onnx_ops.not(X)
    function: null
onnx::OneHot:
    description: "\n    Produces a one-hot tensor based on inputs.\n    The locations\
        \ represented by the index values in the 'indices' input tensor will have\
        \ 'on_value'\n    and the other locations will have 'off_value' in the output\
        \ tensor, where 'on_value' and 'off_value'\n    are specified as part of required\
        \ input argument 'values', which is a two-element tensor of format\n    [off_value,\
        \ on_value]. The rank of the output tensor will be one greater than the rank\
        \ of the\n    input tensor. The additional dimension is for one-hot representation.\
        \ The additional dimension will\n    be inserted at the position specified\
        \ by 'axis'. If 'axis' is not specified then then additional\n    dimension\
        \ will be inserted as the innermost dimension, i.e. axis=-1. The size of the\
        \ additional\n    dimension is specified by required scalar input 'depth'.\
        \ The type of the output tensor is the same\n    as the type of the 'values'\
        \ input. Any entries in the 'indices' input tensor with values outside\n \
        \   the range [-depth, depth-1] will result in one-hot representation with\
        \ all 'off_value' values in the\n    output tensor.\n\n    when axis = 0:\n\
        \    output[input[i, j, k], i, j, k] = 1 for all i, j, k and 0 otherwise.\n\
        \n    when axis = -1:\n    output[i, j, k, input[i, j, k]] = 1 for all i,\
        \ j, k and 0 otherwise.\n\n"
    arguments:
    - indices
    - depth
    - values
    expression_string: onnx_ops.onehot(indices, depth, values, axis)
onnx::Optional:
    description: '

        Constructs an optional-type value containing either an empty optional of a
        certain type specified by the attribute,

        or a non-empty value containing the input element.

        '
    arguments:
    - input
    expression_string: onnx_ops.optional(input, type)
onnx::OptionalGetElement:
    description: '

        Outputs the element in the optional-type input. It is an error if the input
        value does not have an element

        and the behavior is undefined in this case.

        '
    arguments:
    - input
    expression_string: onnx_ops.optionalgetelement(input)
onnx::OptionalHasElement:
    description: '

        Returns true if the optional-type input contains an element. If it is an empty
        optional-type, this op returns false.

        '
    arguments:
    - input
    expression_string: onnx_ops.optionalhaselement(input)
onnx::Or:
    description: '

        Returns the tensor resulted from performing the `or` logical operation

        elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting
        support).


        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).

        '
    arguments:
    - A
    - B
    expression_string: onnx_ops.or(A, B)
    function: null
onnx::PRelu:
    description: '

        PRelu takes input data (Tensor<T>) and slope tensor as input, and produces
        one

        output data (Tensor<T>) where the function `f(x) = slope * x for x < 0`,

        `f(x) = x for x >= 0`., is applied to the data tensor elementwise.

        This operator supports **unidirectional broadcasting** (tensor slope should
        be unidirectional broadcastable to input tensor X); for more details please
        check [the doc](Broadcasting.md).'
    arguments:
    - X
    - slope
    expression_string: onnx_ops.prelu(X, slope)
onnx::Pad:
    description: "\nGiven a tensor containing the data to be padded (`data`), a tensor\
        \ containing the number of start and end pad values for axis (`pads`), (optionally)\
        \ a `mode`, and (optionally) `constant_value`,\na padded tensor (`output`)\
        \ is generated.\n\nThe three supported `modes` are (similar to corresponding\
        \ modes supported by `numpy.pad`):\n\n1) `constant`(default) - pads with a\
        \ given constant value as specified by `constant_value` (which defaults to\
        \ 0, empty string, or False)\n\n2) `reflect` - pads with the reflection of\
        \ the vector mirrored on the first and last values of the vector along each\
        \ axis\n\n3) `edge` - pads with the edge values of array\n\n\nExample 1 (`constant`\
        \ mode):\n  Insert 0 pads to the beginning of the second dimension.\n\n  data\
        \ =\n  [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n\n\
        \  pads = [0, 2, 0, 0]\n\n  mode = 'constant'\n\n  constant_value = 0.0\n\n\
        \  output =\n  [\n      [0.0, 0.0, 1.0, 1.2],\n      [0.0, 0.0, 2.3, 3.4],\n\
        \      [0.0, 0.0, 4.5, 5.7],\n  ]\n\n\nExample 2 (`reflect` mode):\n  data\
        \ =\n  [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n\n\
        \  pads = [0, 2, 0, 0]\n\n  mode = 'reflect'\n\n  output =\n  [\n      [1.0,\
        \ 1.2, 1.0, 1.2],\n      [2.3, 3.4, 2.3, 3.4],\n      [4.5, 5.7, 4.5, 5.7],\n\
        \  ]\n\n\nExample 3 (`edge` mode):\n  data =\n  [\n      [1.0, 1.2],\n   \
        \   [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n\n  pads = [0, 2, 0, 0]\n\n  mode\
        \ = 'edge'\n\n  output =\n  [\n      [1.0, 1.0, 1.0, 1.2],\n      [2.3, 2.3,\
        \ 2.3, 3.4],\n      [4.5, 4.5, 4.5, 5.7],\n  ]\n\n"
    arguments:
    - data
    - pads
    - constant_value
    expression_string: onnx_ops.pad(data, pads, constant_value, mode)
onnx::Pow:
    description: '

        Pow takes input data (Tensor<T>) and exponent Tensor, and

        produces one output data (Tensor<T>) where the function `f(x) = x^exponent`,

        is applied to the data tensor elementwise.

        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).'
    arguments:
    - X
    - Y
    expression_string: onnx_ops.pow(X, Y)
onnx::QLinearConv:
    description: '

        The convolution operator consumes a quantized input tensor, its scale and
        zero point,

        a quantized filter, its scale and zero point, and output''s scale and zero
        point,

        and computes the quantized output. Each scale and zero-point pair must have
        same shape.

        It means they must be either scalars (per tensor) or 1-D tensors (per output
        channel).

        Each input or output and its related zero point must have same type.

        When bias is present it must be quantized using scale = input scale * weight
        scale and

        zero point as 0.

        '
    arguments:
    - x
    - x_scale
    - x_zero_point
    - w
    - w_scale
    - w_zero_point
    - y_scale
    - y_zero_point
    - B
    expression_string: onnx_ops.qlinearconv(x, x_scale, x_zero_point, w, w_scale,
        w_zero_point, y_scale, y_zero_point, B, auto_pad, dilations, group, kernel_shape,
        pads, strides)
onnx::QLinearMatMul:
    description: '

        Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html.

        It consumes two quantized input tensors, their scales and zero points, scale
        and zero point of output,

        and computes the quantized output. The quantization formula is y = saturate((x
        / y_scale) + y_zero_point).

        For (x / y_scale), it is rounding to nearest ties to even. Refer to https://en.wikipedia.org/wiki/Rounding
        for details.

        Scale and zero point must have same shape. They must be either scalar (per
        tensor) or N-D tensor

        (per row for ''a'' and per column for ''b''). Scalar refers to per tensor
        quantization whereas N-D refers to per row

        or per column quantization. If the input is 2D of shape [M, K] then zero point
        and scale tensor may be

        an M element vector [v_1, v_2, ..., v_M] for per row quantization and K element
        vector of shape [v_1, v_2, ..., v_K]

        for per column quantization. If the input is N-D tensor with shape [D1, D2,
        M, K] then zero point and scale tensor may

        have shape [D1, D2, M, 1] for per row quantization and shape [D1, D2, 1, K]
        for per column quantization.

        Production must never overflow, and accumulation may overflow if and only
        if in 32 bits.

        '
    arguments:
    - a
    - a_scale
    - a_zero_point
    - b
    - b_scale
    - b_zero_point
    - y_scale
    - y_zero_point
    expression_string: onnx_ops.qlinearmatmul(a, a_scale, a_zero_point, b, b_scale,
        b_zero_point, y_scale, y_zero_point)
onnx::QuantizeLinear:
    description: '

        The linear quantization operator. It consumes a high precision tensor, a scale,
        and a zero point to compute the low precision / quantized tensor.

        The scale factor and zero point must have same shape, and can be either a
        scalar for per-tensor / per layer quantization, or a 1-D tensor for per-axis
        quantization.

        The quantization formula is y = saturate ((x / y_scale) + y_zero_point).

        For saturation, it saturates to [0, 255] if it''s uint8, or [-128, 127] if
        it''s int8.

        For (x / y_scale), it''s rounding to nearest ties to even. Refer to https://en.wikipedia.org/wiki/Rounding
        for details. ''y_zero_point'' and ''y'' must have same type.

        '
    arguments:
    - x
    - y_scale
    - y_zero_point
    expression_string: onnx_ops.quantizelinear(x, y_scale, y_zero_point, axis)
onnx::RNN:
    description: "\nComputes an one-layer simple RNN. This operator is usually supported\n\
        via some custom implementation such as CuDNN.\n\nNotations:\n\n`X` - input\
        \ tensor\n\n`i` - input gate\n\n`t` - time step (t-1 means previous time step)\n\
        \n`Wi` - W parameter weight matrix for input gate\n\n`Ri` - R recurrence weight\
        \ matrix for input gate\n\n`Wbi` - W parameter bias vector for input gate\n\
        \n`Rbi` - R parameter bias vector for input gate\n\n`WBi` - W parameter weight\
        \ matrix for backward input gate\n\n`RBi` - R recurrence weight matrix for\
        \ backward input gate\n\n`WBbi` - WR bias vectors for backward input gate\n\
        \n`RBbi` - RR bias vectors for backward input gate\n\n`H` - Hidden state\n\
        \n`num_directions` - 2 if direction == bidirectional else 1\n\nActivation\
        \ functions:\n\n  Relu(x)                - max(0, x)\n\n  Tanh(x)        \
        \        - (1 - e^{-2x})/(1 + e^{-2x})\n\n  Sigmoid(x)             - 1/(1\
        \ + e^{-x})\n\n  (NOTE: Below are optional)\n\n  Affine(x)              -\
        \ alpha*x + beta\n\n  LeakyRelu(x)           - x if x >= 0 else alpha * x\n\
        \n  ThresholdedRelu(x)     - x if x >= alpha else 0\n\n  ScaledTanh(x)   \
        \       - alpha*Tanh(beta*x)\n\n  HardSigmoid(x)         - min(max(alpha*x\
        \ + beta, 0), 1)\n\n  Elu(x)                 - x if x >= 0 else alpha*(e^x\
        \ - 1)\n\n  Softsign(x)            - x/(1 + |x|)\n\n  Softplus(x)        \
        \    - log(1 + e^x)\n\nEquations (Default: f=Tanh):\n\n  - Ht = f(Xt*(Wi^T)\
        \ + Ht-1*(Ri^T) + Wbi + Rbi)\nThis operator has **optional** inputs/outputs.\
        \ See [the doc](IR.md) for more details about the representation of optional\
        \ arguments. An empty string may be used in the place of an actual argument's\
        \ name to indicate a missing argument. Trailing optional arguments (those\
        \ not followed by an argument that is present) may also be simply omitted.\n"
    arguments:
    - X
    - W
    - R
    - B
    - sequence_lens
    - initial_h
    expression_string: onnx_ops.rnn(X, W, R, B, sequence_lens, initial_h, activation_alpha,
        activation_beta, activations, clip, direction, hidden_size, layout)
onnx::RandomNormal:
    description: '

        Generate a tensor with random values drawn from a normal distribution. The
        shape

        of the tensor is specified by the `shape` argument and the parameter of the
        normal distribution

        specified by `mean` and `scale`.


        The data type is specified by the ''dtype'' argument. The ''dtype'' argument
        must

        be one of the data types specified in the ''DataType'' enum field in the

        TensorProto message.

        '
    arguments: []
    expression_string: onnx_ops.randomnormal(dtype, mean, scale, seed, shape)
onnx::RandomNormalLike:
    description: '

        Generate a tensor with random values drawn from a normal distribution.

        The shape of the output tensor is copied from the shape of the input tensor,

        and the parameters of the normal distribution are specified by `mean` and
        `scale`.


        The data type is specified by the ''dtype'' argument, or copied from the input
        tensor if not provided.

        The ''dtype'' argument must be one of the data types specified in the ''DataType''
        enum field in the

        TensorProto message, and be valid as an output type.

        '
    arguments:
    - input
    expression_string: onnx_ops.randomnormallike(input, dtype, mean, scale, seed)
onnx::RandomUniform:
    description: '

        Generate a tensor with random values drawn from a uniform distribution. The
        shape

        of the tensor is specified by the `shape` argument and the range by `low`
        and `high`.


        The data type is specified by the ''dtype'' argument. The ''dtype'' argument
        must

        be one of the data types specified in the ''DataType'' enum field in the

        TensorProto message.

        '
    arguments: []
    expression_string: onnx_ops.randomuniform(dtype, high, low, seed, shape)
onnx::RandomUniformLike:
    description: '

        Generate a tensor with random values drawn from a uniform distribution.

        The shape of the output tensor is copied from the shape of the input tensor,

        and the parameters of the uniform distribution are specified by `low` and
        `high`.


        The data type is specified by the ''dtype'' argument, or copied from the input
        tensor if not provided.

        The ''dtype'' argument must be one of the data types specified in the ''DataType''
        enum field in the

        TensorProto message and be valid as an output type.

        '
    arguments:
    - input
    expression_string: onnx_ops.randomuniformlike(input, dtype, high, low, seed)
onnx::Range:
    description: '

        Generate a tensor containing a sequence of numbers that begin at `start` and
        extends by increments of `delta`

        up to `limit` (exclusive).


        The number of elements in the output of range is computed as below-


        `number_of_elements = max( ceil( (limit - start) / delta ) , 0 )`


        The pseudocode determining the contents of the output is shown below-


        `for(int i=0; i<number_of_elements; ++i)`


        `{`


        `    output[i] =  start + (i * delta);  `


        `}`


        `Example 1`

        Inputs: start = 3, limit = 9, delta = 3

        Output: [3, 6]


        `Example 2`

        Inputs: start = 10, limit = 4, delta = -2

        Output: [10, 8, 6]


        '
    arguments:
    - start
    - limit
    - delta
    expression_string: onnx_ops.range(start, limit, delta)
onnx::Reciprocal:
    description: '

        Reciprocal takes one input data (Tensor<T>) and produces one output data

        (Tensor<T>) where the reciprocal is, y = 1/x, is applied to

        the tensor elementwise.

        '
    arguments:
    - X
    expression_string: onnx_ops.reciprocal(X)
onnx::ReduceL1:
    description: '

        Computes the L1 norm of the input tensor''s element along the provided axes.
        The resulting

        tensor has the same rank as the input if keepdims equals 1. If keepdims equals
        0, then

        the resulting tensor has the reduced dimension pruned.


        The above behavior is similar to numpy, with the exception that numpy defaults
        keepdims to

        False instead of True.'
    arguments:
    - data
    expression_string: onnx_ops.reducel1(data, axes, keepdims)
onnx::ReduceL2:
    description: '

        Computes the L2 norm of the input tensor''s element along the provided axes.
        The resulting

        tensor has the same rank as the input if keepdims equals 1. If keepdims equals
        0, then

        the resulting tensor has the reduced dimension pruned.


        The above behavior is similar to numpy, with the exception that numpy defaults
        keepdims to

        False instead of True.'
    arguments:
    - data
    expression_string: onnx_ops.reducel2(data, axes, keepdims)
onnx::ReduceLogSum:
    description: '

        Computes the log sum of the input tensor''s element along the provided axes.
        The resulting

        tensor has the same rank as the input if keepdims equals 1. If keepdims equals
        0, then

        the resulting tensor has the reduced dimension pruned.


        The above behavior is similar to numpy, with the exception that numpy defaults
        keepdims to

        False instead of True.'
    arguments:
    - data
    expression_string: onnx_ops.reducelogsum(data, axes, keepdims)
onnx::ReduceLogSumExp:
    description: '

        Computes the log sum exponent of the input tensor''s element along the provided
        axes. The resulting

        tensor has the same rank as the input if keepdims equals 1. If keepdims equals
        0, then

        the resulting tensor has the reduced dimension pruned.


        The above behavior is similar to numpy, with the exception that numpy defaults
        keepdims to

        False instead of True.'
    arguments:
    - data
    expression_string: onnx_ops.reducelogsumexp(data, axes, keepdims)
onnx::ReduceMax:
    description: '

        Computes the max of the input tensor''s element along the provided axes. The
        resulting

        tensor has the same rank as the input if keepdims equals 1. If keepdims equals
        0, then

        the resulting tensor has the reduced dimension pruned.


        The above behavior is similar to numpy, with the exception that numpy defaults
        keepdims to

        False instead of True.'
    arguments:
    - data
    expression_string: onnx_ops.reducemax(data, axes, keepdims)
onnx::ReduceMean:
    description: '

        Computes the mean of the input tensor''s element along the provided axes.
        The resulting

        tensor has the same rank as the input if keepdims equals 1. If keepdims equals
        0, then

        the resulting tensor has the reduced dimension pruned.


        The above behavior is similar to numpy, with the exception that numpy defaults
        keepdims to

        False instead of True.'
    arguments:
    - data
    expression_string: onnx_ops.reducemean(data, axes, keepdims)
onnx::ReduceMin:
    description: '

        Computes the min of the input tensor''s element along the provided axes. The
        resulting

        tensor has the same rank as the input if keepdims equals 1. If keepdims equals
        0, then

        the resulting tensor has the reduced dimension pruned.


        The above behavior is similar to numpy, with the exception that numpy defaults
        keepdims to

        False instead of True.'
    arguments:
    - data
    expression_string: onnx_ops.reducemin(data, axes, keepdims)
onnx::ReduceProd:
    description: '

        Computes the product of the input tensor''s element along the provided axes.
        The resulting

        tensor has the same rank as the input if keepdims equals 1. If keepdims equals
        0, then

        the resulting tensor has the reduced dimension pruned.


        The above behavior is similar to numpy, with the exception that numpy defaults
        keepdims to

        False instead of True.'
    arguments:
    - data
    expression_string: onnx_ops.reduceprod(data, axes, keepdims)
onnx::ReduceSum:
    description: '

        Computes the sum of the input tensor''s element along the provided axes. The
        resulting

        tensor has the same rank as the input if keepdims equals 1. If keepdims equals
        0, then

        the resulting tensor has the reduced dimension pruned.


        The above behavior is similar to numpy, with the exception that numpy defaults
        keepdims to

        False instead of True.'
    arguments:
    - data
    - axes
    expression_string: onnx_ops.reducesum(data, axes, keepdims, noop_with_empty_axes)
onnx::ReduceSumSquare:
    description: '

        Computes the sum square of the input tensor''s element along the provided
        axes. The resulting

        tensor has the same rank as the input if keepdims equals 1. If keepdims equals
        0, then

        the resulting tensor has the reduced dimension pruned.


        The above behavior is similar to numpy, with the exception that numpy defaults
        keepdims to

        False instead of True.'
    arguments:
    - data
    expression_string: onnx_ops.reducesumsquare(data, axes, keepdims)
onnx::Relu:
    description: '

        Relu takes one input data (Tensor<T>) and produces one output data

        (Tensor<T>) where the rectified linear function, y = max(0, x), is applied
        to

        the tensor elementwise.

        '
    arguments:
    - X
    expression_string: onnx_ops.relu(X)
onnx::Reshape:
    description: '

        Reshape the input tensor similar to numpy.reshape.

        First input is the data tensor, second input is a shape tensor which specifies
        the output shape. It outputs the reshaped tensor.

        At most one dimension of the new shape can be -1. In this case, the value
        is

        inferred from the size of the tensor and the remaining dimensions. A dimension

        could also be 0, in which case the actual dimension value is unchanged (i.e.
        taken

        from the input tensor). If ''allowzero'' is set, and the new shape includes
        0, the

        dimension will be set explicitly to zero (i.e. not taken from input tensor).

        Shape (second input) could be an empty shape, which means converting to a
        scalar.

        The input tensor''s shape and the output tensor''s shape are required to have
        the same number of elements.


        If the attribute ''allowzero'' is set, it is invalid for the specified shape
        to

        contain both a zero value and -1, as the value of the dimension corresponding

        to -1 cannot be determined uniquely.

        '
    arguments:
    - data
    - shape
    expression_string: onnx_ops.reshape(data, shape, allowzero)
onnx::Resize:
    description: "\nResize the input tensor. In general, it calculates every value\
        \ in the output tensor as a weighted average of neighborhood (a.k.a. sampling\
        \ locations) in the input tensor.\nEach dimension value of the output tensor\
        \ is:\n  output_dimension = floor(input_dimension * (roi_end - roi_start)\
        \ * scale) if input \\\"sizes\\\" is not specified.\n"
    arguments:
    - X
    - roi
    - scales
    - sizes
    expression_string: onnx_ops.resize(X, roi, scales, sizes, coordinate_transformation_mode,
        cubic_coeff_a, exclude_outside, extrapolation_value, mode, nearest_mode)
onnx::ReverseSequence:
    description: "\nReverse batch of sequences having different lengths specified\
        \ by `sequence_lens`.\n\nFor each slice i iterating on batch axis, the operator\
        \ reverses the first sequence_lens[i] elements on time axis,\nand copies elements\
        \ whose index's beyond sequence_lens[i] to the output. So the output slice\
        \ i contains reversed\nsequences on the first sequence_lens[i] elements, then\
        \ have original values copied for the other elements.\n\nExample 1:\n  input\
        \ = [[0.0, 4.0, 8.0,  12.0],\n           [1.0, 5.0, 9.0,  13.0],\n       \
        \    [2.0, 6.0, 10.0, 14.0],\n           [3.0, 7.0, 11.0, 15.0]]\n  sequence_lens\
        \ = [4, 3, 2, 1]\n  time_axis = 0\n  batch_axis = 1\n\n  output = [[3.0, 6.0,\
        \ 9.0,  12.0],\n            [2.0, 5.0, 8.0,  13.0],\n            [1.0, 4.0,\
        \ 10.0, 14.0],\n            [0.0, 7.0, 11.0, 15.0]]\n\nExample 2:\n  input\
        \ = [[0.0,  1.0,  2.0,  3.0 ],\n           [4.0,  5.0,  6.0,  7.0 ],\n   \
        \        [8.0,  9.0,  10.0, 11.0],\n           [12.0, 13.0, 14.0, 15.0]]\n\
        \  sequence_lens = [1, 2, 3, 4]\n  time_axis = 1\n  batch_axis = 0\n\n  output\
        \ = [[0.0,  1.0,  2.0,  3.0 ],\n            [5.0,  4.0,  6.0,  7.0 ],\n  \
        \          [10.0, 9.0,  8.0,  11.0],\n            [15.0, 14.0, 13.0, 12.0]]\n"
    arguments:
    - input
    - sequence_lens
    expression_string: onnx_ops.reversesequence(input, sequence_lens, batch_axis,
        time_axis)
onnx::RoiAlign:
    description: '

        Region of Interest (RoI) align operation described in the

        [Mask R-CNN paper](https://arxiv.org/abs/1703.06870).

        RoiAlign consumes an input tensor X and region of interests (rois)

        to apply pooling across each RoI; it produces a 4-D tensor of shape

        (num_rois, C, output_height, output_width).


        RoiAlign is proposed to avoid the misalignment by removing

        quantizations while converting from original image into feature

        map and from feature map into RoI feature; in each ROI bin,

        the value of the sampled locations are computed directly

        through bilinear interpolation.

        '
    arguments:
    - X
    - rois
    - batch_indices
    expression_string: onnx_ops.roialign(X, rois, batch_indices, mode, output_height,
        output_width, sampling_ratio, spatial_scale)
onnx::Round:
    description: '

        Round takes one input Tensor and rounds the values, element-wise, meaning

        it finds the nearest integer for each value.

        In case of halfs, the rule is to round them to the nearest even integer.

        The output tensor has the same shape and type as the input.


        Examples:

        ```

        round([0.9]) = [1.0]

        round([2.5]) = [2.0]

        round([2.3]) = [2.0]

        round([1.5]) = [2.0]

        round([-4.5]) = [-4.0]

        ```

        '
    arguments:
    - X
    expression_string: onnx_ops.round(X)
onnx::Scatter:
    description: "\nThis operator is deprecated. Please use ScatterElements, which\
        \ provides the same functionality.\n\nScatter takes three inputs `data`, `updates`,\
        \ and `indices` of the same\nrank r >= 1 and an optional attribute axis that\
        \ identifies an axis of `data`\n(by default, the outer-most axis, that is\
        \ axis 0). The output of the operation\nis produced by creating a copy of\
        \ the input `data`, and then updating its value\nto values specified by `updates`\
        \ at specific index positions specified by\n`indices`. Its output shape is\
        \ the same as the shape of `data`.\n\nFor each entry in `updates`, the target\
        \ index in `data` is obtained by combining\nthe corresponding entry in `indices`\
        \ with the index of the entry itself: the\nindex-value for dimension = axis\
        \ is obtained from the value of the corresponding\nentry in `indices` and\
        \ the index-value for dimension != axis is obtained from the\nindex of the\
        \ entry itself.\n\nFor instance, in a 2-D tensor case, the update corresponding\
        \ to the [i][j] entry\nis performed as below:\n```\n  output[indices[i][j]][j]\
        \ = updates[i][j] if axis = 0,\n  output[i][indices[i][j]] = updates[i][j]\
        \ if axis = 1,\n```\n\nThis operator is the inverse of GatherElements. It\
        \ is similar to Torch's Scatter operation.\n\nExample 1:\n```\n  data = [\n\
        \      [0.0, 0.0, 0.0],\n      [0.0, 0.0, 0.0],\n      [0.0, 0.0, 0.0],\n\
        \  ]\n  indices = [\n      [1, 0, 2],\n      [0, 2, 1],\n  ]\n  updates =\
        \ [\n      [1.0, 1.1, 1.2],\n      [2.0, 2.1, 2.2],\n  ]\n  output = [\n \
        \     [2.0, 1.1, 0.0]\n      [1.0, 0.0, 2.2]\n      [0.0, 2.1, 1.2]\n  ]\n\
        ```\nExample 2:\n```\n  data = [[1.0, 2.0, 3.0, 4.0, 5.0]]\n  indices = [[1,\
        \ 3]]\n  updates = [[1.1, 2.1]]\n  axis = 1\n  output = [[1.0, 1.1, 3.0, 2.1,\
        \ 5.0]]\n```\n"
    arguments:
    - data
    - indices
    - updates
    expression_string: onnx_ops.scatter(data, indices, updates, axis)
onnx::ScatterElements:
    description: "\nScatterElements takes three inputs `data`, `updates`, and `indices`\
        \ of the same\nrank r >= 1 and an optional attribute axis that identifies\
        \ an axis of `data`\n(by default, the outer-most axis, that is axis 0). The\
        \ output of the operation\nis produced by creating a copy of the input `data`,\
        \ and then updating its value\nto values specified by `updates` at specific\
        \ index positions specified by\n`indices`. Its output shape is the same as\
        \ the shape of `data`.\n\nFor each entry in `updates`, the target index in\
        \ `data` is obtained by combining\nthe corresponding entry in `indices` with\
        \ the index of the entry itself: the\nindex-value for dimension = axis is\
        \ obtained from the value of the corresponding\nentry in `indices` and the\
        \ index-value for dimension != axis is obtained from the\nindex of the entry\
        \ itself.\n\nFor instance, in a 2-D tensor case, the update corresponding\
        \ to the [i][j] entry\nis performed as below:\n```\n  output[indices[i][j]][j]\
        \ = updates[i][j] if axis = 0,\n  output[i][indices[i][j]] = updates[i][j]\
        \ if axis = 1,\n```\n\nThis operator is the inverse of GatherElements. It\
        \ is similar to Torch's Scatter operation.\n\nExample 1:\n```\n  data = [\n\
        \      [0.0, 0.0, 0.0],\n      [0.0, 0.0, 0.0],\n      [0.0, 0.0, 0.0],\n\
        \  ]\n  indices = [\n      [1, 0, 2],\n      [0, 2, 1],\n  ]\n  updates =\
        \ [\n      [1.0, 1.1, 1.2],\n      [2.0, 2.1, 2.2],\n  ]\n  output = [\n \
        \     [2.0, 1.1, 0.0]\n      [1.0, 0.0, 2.2]\n      [0.0, 2.1, 1.2]\n  ]\n\
        ```\nExample 2:\n```\n  data = [[1.0, 2.0, 3.0, 4.0, 5.0]]\n  indices = [[1,\
        \ 3]]\n  updates = [[1.1, 2.1]]\n  axis = 1\n  output = [[1.0, 1.1, 3.0, 2.1,\
        \ 5.0]]\n```\n"
    arguments:
    - data
    - indices
    - updates
    expression_string: onnx_ops.scatterelements(data, indices, updates, axis)
onnx::ScatterND:
    description: "\nScatterND takes three inputs `data` tensor of rank r >= 1, `indices`\
        \ tensor of rank q >= 1,\nand `updates` tensor of rank q + r - indices.shape[-1]\
        \ - 1. The output of the operation\nis produced by creating a copy of the\
        \ input `data`, and then updating its value to values\nspecified by `updates`\
        \ at specific index positions specified by `indices`. Its output shape\nis\
        \ the same as the shape of `data`. Note that `indices` should not have duplicate\
        \ entries.\nThat is, two or more `updates` for the same index-location is\
        \ not supported.\n\n`indices` is an integer tensor. Let k denote indices.shape[-1],\
        \ the last dimension in the shape of `indices`.\n `indices` is treated as\
        \ a (q-1)-dimensional tensor of k-tuples, where each k-tuple is a partial-index\
        \ into `data`.\nHence, k can be a value at most the rank of `data`. When k\
        \ equals rank(data), each update entry specifies an\nupdate to a single element\
        \ of the tensor. When k is less than rank(data) each update entry specifies\
        \ an\nupdate to a slice of the tensor. Index values are allowed to be negative,\
        \ as per the usual\nconvention for counting backwards from the end, but are\
        \ expected in the valid range.\n\n`updates` is treated as a (q-1)-dimensional\
        \ tensor of replacement-slice-values. Thus, the\nfirst (q-1) dimensions of\
        \ updates.shape must match the first (q-1) dimensions of indices.shape.\n\
        The remaining dimensions of `updates` correspond to the dimensions of the\n\
        replacement-slice-values. Each replacement-slice-value is a (r-k) dimensional\
        \ tensor,\ncorresponding to the trailing (r-k) dimensions of `data`.  Thus,\
        \ the shape of `updates`\nmust equal indices.shape[0:q-1] ++ data.shape[k:r-1],\
        \ where ++ denotes the concatenation\nof shapes.\n\nThe `output` is calculated\
        \ via the following equation:\n\n    output = np.copy(data)\n    update_indices\
        \ = indices.shape[:-1]\n    for idx in np.ndindex(update_indices):\n     \
        \   output[indices[idx]] = updates[idx]\n\nThe order of iteration in the above\
        \ loop is not specified.\nIn particular, indices should not have duplicate\
        \ entries: that is, if idx1 != idx2, then indices[idx1] != indices[idx2].\n\
        This ensures that the output value does not depend on the iteration order.\n\
        \nThis operator is the inverse of GatherND.\n\nExample 1:\n```\n  data   \
        \ = [1, 2, 3, 4, 5, 6, 7, 8]\n  indices = [[4], [3], [1], [7]]\n  updates\
        \ = [9, 10, 11, 12]\n  output  = [1, 11, 3, 10, 9, 6, 7, 12]\n```\n\nExample\
        \ 2:\n```\n  data    = [[[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3,\
        \ 2, 1]],\n             [[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3,\
        \ 2, 1]],\n             [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6,\
        \ 7, 8]],\n             [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6,\
        \ 7, 8]]]\n  indices = [[0], [2]]\n  updates = [[[5, 5, 5, 5], [6, 6, 6, 6],\
        \ [7, 7, 7, 7], [8, 8, 8, 8]],\n             [[1, 1, 1, 1], [2, 2, 2, 2],\
        \ [3, 3, 3, 3], [4, 4, 4, 4]]]\n  output  = [[[5, 5, 5, 5], [6, 6, 6, 6],\
        \ [7, 7, 7, 7], [8, 8, 8, 8]],\n             [[1, 2, 3, 4], [5, 6, 7, 8],\
        \ [8, 7, 6, 5], [4, 3, 2, 1]],\n             [[1, 1, 1, 1], [2, 2, 2, 2],\
        \ [3, 3, 3, 3], [4, 4, 4, 4]],\n             [[8, 7, 6, 5], [4, 3, 2, 1],\
        \ [1, 2, 3, 4], [5, 6, 7, 8]]]\n```\n"
    arguments:
    - data
    - indices
    - updates
    expression_string: onnx_ops.scatternd(data, indices, updates)
onnx::Selu:
    description: '

        Selu takes one input data (Tensor<T>) and produces one output data

        (Tensor<T>) where the scaled exponential linear unit function,

        `y = gamma * (alpha * e^x - alpha) for x <= 0`, `y = gamma * x for x > 0`,

        is applied to the tensor elementwise.

        '
    arguments:
    - X
    expression_string: onnx_ops.selu(X, alpha, gamma)
onnx::SequenceAt:
    description: '

        Outputs a tensor copy from the tensor at ''position'' in ''input_sequence''.

        Accepted range for ''position'' is in `[-n, n - 1]`, where `n` is the number
        of tensors in ''input_sequence''.

        Negative value means counting positions from the back.

        '
    arguments:
    - input_sequence
    - position
    expression_string: onnx_ops.sequenceat(input_sequence, position)
onnx::SequenceConstruct:
    description: '

        Construct a tensor sequence containing ''inputs'' tensors.

        All tensors in ''inputs'' must have the same data type.

        '
    arguments:
    - inputs
    expression_string: onnx_ops.sequenceconstruct(inputs)
onnx::SequenceEmpty:
    description: '

        Construct an empty tensor sequence, with given data type.

        '
    arguments: []
    expression_string: onnx_ops.sequenceempty(dtype)
onnx::SequenceErase:
    description: '

        Outputs a tensor sequence that removes the tensor at ''position'' from ''input_sequence''.

        Accepted range for ''position'' is in `[-n, n - 1]`, where `n` is the number
        of tensors in ''input_sequence''.

        Negative value means counting positions from the back.

        ''position'' is optional, by default it erases the last tensor from ''input_sequence''.

        '
    arguments:
    - input_sequence
    - position
    expression_string: onnx_ops.sequenceerase(input_sequence, position)
onnx::SequenceInsert:
    description: '

        Outputs a tensor sequence that inserts ''tensor'' into ''input_sequence''
        at ''position''.

        ''tensor'' must have the same data type as ''input_sequence''.

        Accepted range for ''position'' is in `[-n, n]`, where `n` is the number of
        tensors in ''input_sequence''.

        Negative value means counting positions from the back.

        ''position'' is optional, by default it inserts ''tensor'' to the back of
        ''input_sequence''.

        '
    arguments:
    - input_sequence
    - tensor
    - position
    expression_string: onnx_ops.sequenceinsert(input_sequence, tensor, position)
onnx::SequenceLength:
    description: '

        Produces a scalar(tensor of empty shape) containing the number of tensors
        in ''input_sequence''.

        '
    arguments:
    - input_sequence
    expression_string: onnx_ops.sequencelength(input_sequence)
onnx::Shape:
    description: '

        Takes a tensor as input and outputs an 1D int64 tensor containing the shape
        of the input tensor.

        Optional attributes start and end can be used to compute a slice of the input
        tensor''s shape.

        If start axis is omitted, the slice starts from axis 0.

        The end axis, if specified, is exclusive (and the returned value will not
        include the size of that axis).

        If the end axis is omitted, the axes upto the last one will be included.

        Negative axes indicate counting back from the last axis.

        Note that axes will be clamped to the range [0, r-1], where r is the

        rank of the input tensor if they are out-of-range (after adding r in the case
        of

        negative axis). Thus, specifying any end value > r is equivalent to specifying
        an end

        value of r, and specifying any start value < -r is equivalent to specifying
        a start

        value of 0.


        For example:

        Input tensor with shape: [2, 3, 4]

        No attributes specified.

        Output: [2, 3, 4]


        Input tensor with shape: [2, 3, 4]

        start: -1

        Output: [4]


        Input tensor with shape: [2, 3, 4]

        end: -1

        Output: [2, 3]


        Input tensor with shape: [2, 3, 4]

        start: 1

        end: 2

        Output: [3]

        '
    arguments:
    - data
    expression_string: onnx_ops.shape(data, end, start)
onnx::Shrink:
    description: '

        Shrink takes one input data (Tensor<numeric>) and produces one Tensor output,

        having same datatype and shape with input. It has two attributes, lambd and

        bias. The formula of this operator is: If x < -lambd, y = x + bias;

        If x > lambd, y = x - bias; Otherwise, y = 0.

        '
    arguments:
    - input
    expression_string: onnx_ops.shrink(input, bias, lambd)
onnx::Sigmoid:
    description: '

        Sigmoid takes one input data (Tensor<T>) and produces one output data

        (Tensor<T>) where the sigmoid function, y = 1 / (1 + exp(-x)), is applied
        to the

        tensor elementwise.

        '
    arguments:
    - X
    expression_string: onnx_ops.sigmoid(X)
onnx::Sign:
    description: '

        Calculate the sign of the given input tensor element-wise.

        If input > 0, output 1. if input < 0, output -1. if input == 0, output 0.

        '
    arguments:
    - input
    expression_string: onnx_ops.sign(input)
onnx::Sin:
    description: '

        Calculates the sine of the given input tensor, element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.sin(input)
onnx::Sinh:
    description: '

        Calculates the hyperbolic sine of the given input tensor element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.sinh(input)
onnx::Size:
    description: '

        Takes a tensor as input and outputs a int64 scalar that equals to the total
        number of elements of the input tensor.

        '
    arguments:
    - data
    expression_string: onnx_ops.size(data)
onnx::Slice:
    description: "\nProduces a slice of the input tensor along multiple axes. Similar\
        \ to numpy:\nhttps://numpy.org/doc/stable/user/basics.indexing.html?highlight=slice#slicing-and-striding\n\
        \nSlice uses the `starts`, `ends`, `axes` and `steps` inputs to select a sub-tensor\n\
        of its input `data` tensor.\n\nAn effective `start[i]`, `end[i]`, and `step[i]`\
        \ must be computed for each `i`\nin `[0, ... r-1]` where `r = rank(input)`\
        \ as follows:\n\nIf `axes` are omitted, they are set to `[0, ..., r-1]`.\n\
        If `steps` are omitted, they are set to `[1, ..., 1]` of length `len(starts)`\n\
        \nThe effective values are initialized as `start[i] = 0`, `end[i] = dims[i]`\
        \ where\n`dims` are the dimensions of `input` and `step[i] = `1.\n\nAll negative\
        \ elements of `axes` are made non-negatve by adding `r` to them, where\n`r\
        \ =rank(input)`.\n\nAll negative values in `starts[i]` and `ends[i]` have\
        \ `dims[axes[i]]` added to them,\nwhere `dims` are the dimensions of `input`.\
        \ Then `start[axes[i]]` is the adjusted\n`starts[i]` is clamped into the range\
        \ `[0, dims[axes[i]]]` for positive stepping\nand `[0, dims[axes[i]]-1]` for\
        \ negative stepping.\n\nThe clamping for the adjusted `ends[i]` depends on\
        \ the sign of `steps[i]` and must\naccommodate copying 0 through `dims[axes[i]]`\
        \ elements, so for positive stepping\n`end[axes[i]]` is clamped to `[0, dims[axes[i]]]`,\
        \ while for negative stepping it\nis clamped to `[-1, dims[axes[i]]-1]`.\n\
        \nFinally, `step[axes[i]] = steps[i]`.\n\nFor slicing to the end of a dimension\
        \ with unknown size, it is recommended to pass\nin `INT_MAX` when slicing\
        \ forward and 'INT_MIN' when slicing backward.\n\nExample 1:\n  data = [\n\
        \      [1, 2, 3, 4],\n      [5, 6, 7, 8],\n  ]\n  axes = [0, 1]\n  starts\
        \ = [1, 0]\n  ends = [2, 3]\n  steps = [1, 2]\n  result = [\n      [5, 7],\n\
        \  ]\nExample 2:\n  data = [\n      [1, 2, 3, 4],\n      [5, 6, 7, 8],\n \
        \ ]\n  starts = [0, 1]\n  ends = [-1, 1000]\n  result = [\n      [2, 3, 4],\n\
        \  ]\n"
    arguments:
    - data
    - starts
    - ends
    - axes
    - steps
    expression_string: onnx_ops.slice(data, starts, ends, axes, steps)
onnx::Softmax:
    description: "\nThe operator computes the normalized exponential values for the\
        \ given input:\n\n Softmax(input, axis) = Exp(input) / ReduceSum(Exp(input),\
        \ axis=axis, keepdims=1) \n\nThe \"axis\" attribute indicates the dimension\
        \ along which Softmax\nwill be performed. The output tensor has the same shape\n\
        and contains the Softmax values of the corresponding input.\n"
    arguments:
    - input
    expression_string: onnx_ops.softmax(input, axis)
onnx::SoftmaxCrossEntropyLoss:
    description: "Loss function that measures the softmax cross entropy\nbetween 'scores'\
        \ and 'labels'.\nThis operator first computes a loss tensor whose shape is\
        \ identical to the labels input.\nIf the input is 2-D with shape (N, C), the\
        \ loss tensor may be a N-element vector L = (l_1, l_2, ..., l_N).\nIf the\
        \ input is N-D tensor with shape (N, C, D1, D2, ..., Dk),\nthe loss tensor\
        \ L may have (N, D1, D2, ..., Dk) as its shape and L[i,][j_1][j_2]...[j_k]\
        \ denotes a scalar element in L.\nAfter L is available, this operator can\
        \ optionally do a reduction operator.\n\nshape(scores): (N, C) where C is\
        \ the number of classes, or (N, C, D1, D2,..., Dk),\n        with K >= 1 in\
        \ case of K-dimensional loss.\nshape(labels): (N) where each value is 0 <=\
        \ labels[i] <= C-1, or (N, D1, D2,..., Dk),\n        with K >= 1 in case of\
        \ K-dimensional loss.\n\nThe loss for one sample, l_i, can caculated as follows:\n\
        \    l[i][d1][d2]...[dk] = -y[i][c][d1][d2]..[dk], where i is the index of\
        \ classes.\nor\n    l[i][d1][d2]...[dk] = -y[i][c][d1][d2]..[dk] * weights[c],\
        \ if 'weights' is provided.\n\nloss is zero for the case when label-value\
        \ equals ignore_index.\n    l[i][d1][d2]...[dk]  = 0, when labels[n][d1][d2]...[dk]\
        \ = ignore_index\n\nwhere:\n    p = Softmax(scores)\n    y = Log(p)\n    c\
        \ = labels[i][d1][d2]...[dk]\n\nFinally, L is optionally reduced:\nIf reduction\
        \ = 'none', the output is L with shape (N, D1, D2, ..., Dk).\nIf reduction\
        \ = 'sum', the output is scalar: Sum(L).\nIf reduction = 'mean', the output\
        \ is scalar: ReduceMean(L), or if weight is provided: ReduceSum(L) / ReduceSum(W),\n\
        where tensor W is of shape (N, D1, D2, ..., Dk) and W[n][d1][d2]...[dk] =\
        \ weights[labels[i][d1][d2]...[dk]].\n"
    arguments:
    - scores
    - labels
    - weights
    expression_string: onnx_ops.softmaxcrossentropyloss(scores, labels, weights, ignore_index,
        reduction)
onnx::Softplus:
    description: '

        Softplus takes one input data (Tensor<T>) and produces one output data

        (Tensor<T>) where the softplus function, y = ln(exp(x) + 1), is applied to

        the tensor elementwise.

        '
    arguments:
    - X
    expression_string: onnx_ops.softplus(X)
onnx::Softsign:
    description: '

        Calculates the softsign (x/(1+|x|)) of the given input tensor element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.softsign(input)
onnx::SpaceToDepth:
    description: 'SpaceToDepth rearranges blocks of spatial data into depth. More
        specifically,

        this op outputs a copy of the input tensor where values from the height and
        width dimensions

        are moved to the depth dimension.

        '
    arguments:
    - input
    expression_string: onnx_ops.spacetodepth(input, blocksize)
onnx::Split:
    description: 'Split a tensor into a list of tensors, along the specified

        ''axis''. Lengths of the parts can be specified using input ''split''.

        Otherwise, the tensor is split to equal sized parts.

        '
    arguments:
    - input
    - split
    expression_string: onnx_ops.split(input, split, axis)
onnx::SplitToSequence:
    description: 'Split a tensor into a sequence of tensors, along the specified

        ''axis''. Lengths of the parts can be specified using argument ''split''.

        ''split'' must contain only positive numbers.

        ''split'' is either a scalar (tensor of empty shape), or a 1-D tensor.

        If ''split'' is a scalar, then ''input'' will be split into equally sized
        chunks(if possible).

        Last chunk will be smaller if the ''input'' size along the given axis ''axis''
        is not divisible

        by ''split''.

        Otherwise, the tensor is split into ''size(split)'' chunks, with lengths of
        the parts on ''axis''

        specified in ''split''. In this scenario, the sum of entries in ''split''
        must be equal to the

        dimension size of input tensor on ''axis''.

        '
    arguments:
    - input
    - split
    expression_string: onnx_ops.splittosequence(input, split, axis, keepdims)
onnx::Sqrt:
    description: '

        Square root takes one input data (Tensor<T>) and produces one output data

        (Tensor<T>) where the square root is, y = x^0.5, is applied to

        the tensor elementwise. If x is negative, then it will return NaN.

        '
    arguments:
    - X
    expression_string: onnx_ops.sqrt(X)
onnx::Squeeze:
    description: '

        Remove single-dimensional entries from the shape of a tensor.

        Takes an input `axes` with a list of axes to squeeze.

        If `axes` is not provided, all the single dimensions will be removed from

        the shape. If an axis is selected with shape entry not equal to one, an error
        is raised.

        '
    arguments:
    - data
    - axes
    expression_string: onnx_ops.squeeze(data, axes)
onnx::StringNormalizer:
    description: '

        StringNormalization performs string operations for basic cleaning.

        This operator has only one input (denoted by X) and only one output

        (denoted by Y). This operator first examines the elements in the X,

        and removes elements specified in "stopwords" attribute.

        After removing stop words, the intermediate result can be further lowercased,

        uppercased, or just returned depending the "case_change_action" attribute.

        This operator only accepts [C]- and [1, C]-tensor.

        If all elements in X are dropped, the output will be the empty value of string
        tensor with shape [1]

        if input shape is [C] and shape [1, 1] if input shape is [1, C].

        '
    arguments:
    - X
    expression_string: onnx_ops.stringnormalizer(X, case_change_action, is_case_sensitive,
        locale, stopwords)
onnx::Sub:
    description: '

        Performs element-wise binary subtraction (with Numpy-style broadcasting support).


        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).


        (Opset 14 change): Extend supported types to include uint8, int8, uint16,
        and int16.

        '
    arguments:
    - A
    - B
    expression_string: onnx_ops.sub(A, B)
onnx::Sum:
    description: '

        Element-wise sum of each of the input tensors (with Numpy-style broadcasting
        support).

        All inputs and outputs must have the same data type.

        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).

        '
    arguments:
    - data_0
    expression_string: onnx_ops.sum(data_0)
onnx::Tan:
    description: '

        Calculates the tangent of the given input tensor, element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.tan(input)
onnx::Tanh:
    description: '

        Calculates the hyperbolic tangent of the given input tensor element-wise.

        '
    arguments:
    - input
    expression_string: onnx_ops.tanh(input)
onnx::TfIdfVectorizer:
    description: '

        This transform extracts n-grams from the input sequence and save them as a
        vector. Input can

        be either a 1-D or 2-D tensor. For 1-D input, output is the n-gram representation
        of that input.

        For 2-D input, the output is also a  2-D tensor whose i-th row is the n-gram
        representation of the i-th input row.

        More specifically, if input shape is [C], the corresponding output shape would
        be [max(ngram_indexes) + 1].

        If input shape is [N, C], this operator produces a [N, max(ngram_indexes)
        + 1]-tensor.


        In contrast to standard n-gram extraction, here, the indexes of extracting
        an n-gram from the original

        sequence are not necessarily consecutive numbers. The discontinuity between
        indexes are controlled by the number of skips.

        If the number of skips is 2, we should skip two tokens when scanning through
        the original sequence.

        Let''s consider an example. Assume that input sequence is [94, 17, 36, 12,
        28] and the number of skips is 2.

        The associated 2-grams are [94, 12] and [17, 28] respectively indexed by [0,
        3] and [1, 4].

        If the number of skips becomes 0, the 2-grams generated are [94, 17], [17,
        36], [36, 12], [12, 28]

        indexed by [0, 1], [1, 2], [2, 3], [3, 4], respectively.


        The output vector (denoted by Y) stores the count of each n-gram;

        Y[ngram_indexes[i]] indicates the times that the i-th n-gram is found. The
        attribute ngram_indexes is used to determine the mapping

        between index i and the corresponding n-gram''s output coordinate. If pool_int64s
        is [94, 17, 17, 36], ngram_indexes is [1, 0],

        ngram_counts=[0, 0], then the Y[0] (first element in Y) and Y[1] (second element
        in Y) are the counts of [17, 36] and [94, 17],

        respectively. An n-gram which cannot be found in pool_strings/pool_int64s
        should be ignored and has no effect on the output.

        Note that we may consider all skips up to S when generating the n-grams.


        The examples used above are true if mode is "TF". If mode is "IDF", all the
        counts larger than 1 would be truncated to 1 and

        the i-th element in weights would be used to scale (by multiplication) the
        count of the i-th n-gram in pool. If mode is "TFIDF",

        this operator first computes the counts of all n-grams and then scale them
        by the associated values in the weights attribute.


        Only one of pool_strings and pool_int64s can be set. If pool_int64s is set,
        the input should be an integer tensor.

        If pool_strings is set, the input must be a string tensor.

        '
    arguments:
    - X
    expression_string: onnx_ops.tfidfvectorizer(X, max_gram_length, max_skip_count,
        min_gram_length, mode, ngram_counts, ngram_indexes, pool_int64s, pool_strings,
        weights)
onnx::ThresholdedRelu:
    description: '

        ThresholdedRelu takes one input data (Tensor<T>) and produces one output data

        (Tensor<T>) where the rectified linear function, y = x for x > alpha, y =
        0 otherwise,

        is applied to the tensor elementwise.

        '
    arguments:
    - X
    expression_string: onnx_ops.thresholdedrelu(X, alpha)
onnx::Tile:
    description: 'Constructs a tensor by tiling a given tensor.

        This is the same as function `tile` in Numpy, but no broadcast.

        For example A = [[1, 2], [3, 4]], B = [1, 2], tile(A, B) = [[1, 2, 1, 2],
        [3, 4, 3, 4]]

        '
    arguments:
    - input
    - repeats
    expression_string: onnx_ops.tile(input, repeats)
onnx::TopK:
    description: "\nRetrieve the top-K largest or smallest elements along a specified\
        \ axis. Given an input tensor of\nshape [a_1, a_2, ..., a_n, r] and integer\
        \ argument k, return two outputs:\n  -Value tensor of shape [a_1, a_2, ...,\
        \ a_{axis-1}, k, a_{axis+1}, ... a_n]\n    which contains the values of the\
        \ top k elements along the specified axis\n  -Index tensor of shape [a_1,\
        \ a_2, ..., a_{axis-1}, k, a_{axis+1}, ... a_n] which\n   contains the indices\
        \ of the top k elements (original indices from the input\n   tensor).\n\n\
        If \"largest\" is 1 (the default value) then the k largest elements are returned.\n\
        If \"sorted\" is 1 (the default value) then the resulting k elements will\
        \ be sorted.\nIf \"sorted\" is 0, order of returned 'Values' and 'Indices'\
        \ are undefined.\n\nGiven two equivalent values, this operator uses the indices\
        \ along the axis as\n a tiebreaker. That is, the element with the lower index\
        \ will appear first.\n"
    arguments:
    - X
    - K
    expression_string: onnx_ops.topk(X, K, axis, largest, sorted)
onnx::Transpose:
    description: '

        Transpose the input tensor similar to numpy.transpose. For example, when

        perm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape

        will be (2, 1, 3).

        '
    arguments:
    - data
    expression_string: onnx_ops.transpose(data, perm)
onnx::Trilu:
    description: '

        Given a 2-D matrix or batches of 2-D matrices, returns the upper or lower
        triangular part of the tensor(s).

        The attribute "upper" determines whether the upper or lower part is retained.
        If set to true,

        the upper triangular matrix is retained. Lower triangular matrix is retained
        otherwise.

        Default value for the "upper" attribute is true.

        Trilu takes one input tensor of shape [*, N, M], where * is zero or more batch
        dimensions. The upper triangular part consists

        of the elements on and above the given diagonal (k). The lower triangular
        part consists of elements on and below the diagonal.

        All other elements in the matrix are set to zero.

        If k = 0, the triangular part on and above/below the main diagonal is retained.

        If upper is set to true, a positive k retains the upper triangular matrix
        excluding the main diagonal and (k-1) diagonals above it.

        A negative k value retains the main diagonal and |k| diagonals below it.

        If upper is set to false, a positive k retains the lower triangular matrix
        including the main diagonal and k diagonals above it.

        A negative k value excludes the main diagonal and (|k|-1) diagonals below
        it.

        '
    arguments:
    - input
    - k
    expression_string: onnx_ops.trilu(input, k, upper)
onnx::Unique:
    description: "\nFind the unique elements of a tensor. When an optional attribute\
        \ 'axis' is provided, unique subtensors sliced along the 'axis' are returned.\n\
        Otherwise the input tensor is flattened and unique values of the flattened\
        \ tensor are returned.\n\nThis operator returns the unique values or sliced\
        \ unique subtensors of the input tensor and three optional outputs.\nThe first\
        \ output tensor 'Y' contains all unique values or subtensors of the input.\n\
        The second optional output tensor 'indices' contains indices of 'Y' elements'\
        \ first occurance in 'X'..\nThe third optional output tensor 'inverse_indices'\
        \ contains, for elements of 'X', its corresponding indices in 'Y'. \".\nThe\
        \ fourth optional output tensor 'counts' contains the count of each element\
        \ of 'Y' in the input.\n\nOutputs are either sorted in ascending order or\
        \ optionally in the order of the first occurrence of the values in the input.\n\
        \nhttps://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html\n\
        \nExample 1:\n  input_X = [2, 1, 1, 3, 4, 3]\n  attribute_sorted = 0\n  attribute_axis\
        \ = None\n  output_Y = [2, 1, 3, 4]\n  output_indices = [0, 1, 3, 4]\n  output_inverse_indices\
        \ = [0, 1, 1, 2, 3, 2]\n  output_counts = [1, 2, 2, 1]\n\nExample 2:\n  input_X\
        \ = [[1, 3], [2, 3]]\n  attribute_sorted = 1\n  attribute_axis = None\n  output_Y\
        \ = [1, 2, 3]\n  output_indices = [0, 2, 1]\n  output_inverse_indices = [0,\
        \ 2, 1, 2]\n  output_counts = [1, 1, 2]\n\nExample 3:\n  input_X = [[1, 0,\
        \ 0], [1, 0, 0], [2, 3, 4]]\n  attribute_sorted = 1\n  attribute_axis = 0\n\
        \  output_Y = [[1, 0, 0], [2, 3, 4]]\n  output_indices = [0, 2]\n  output_inverse_indices\
        \ = [0, 0, 1]\n  output_counts = [2, 1]\n\nExample 4:\n  input_x = [[[1.,\
        \ 1.], [0., 1.], [2., 1.], [0., 1.]],\n             [[1., 1.], [0., 1.], [2.,\
        \ 1.], [0., 1.]]]\n  attribute_sorted = 1\n  attribute_axis = 1\n\n  intermediate\
        \ data are presented below for better understanding:\n\n  there are 4 subtensors\
        \ sliced along axis 1 of input_x (shape = (2, 4, 2)):\n  A: [[1, 1], [1, 1]],\n\
        \     [[0, 1], [0, 1]],\n     [[2, 1], [2, 1]],\n     [[0, 1], [0, 1]].\n\n\
        \  there are 3 unique subtensors:\n  [[1, 1], [1, 1]],\n  [[0, 1], [0, 1]],\n\
        \  [[2, 1], [2, 1]].\n\n  sorted unique subtensors:\n  B: [[0, 1], [0, 1]],\n\
        \     [[1, 1], [1, 1]],\n     [[2, 1], [2, 1]].\n\n  output_Y is constructed\
        \ from B:\n  [[[0. 1.], [1. 1.], [2. 1.]],\n   [[0. 1.], [1. 1.], [2. 1.]]]\n\
        \n  output_indices is to map from B to A:\n  [1, 0, 2]\n\n  output_inverse_indices\
        \ is to map from A to B:\n  [1, 0, 2, 0]\n\n  output_counts = [2 1 1]\n"
    arguments:
    - X
    expression_string: onnx_ops.unique(X, axis, sorted)
onnx::Unsqueeze:
    description: "\nInsert single-dimensional entries to the shape of an input tensor\
        \ (`data`).\nTakes one required input `axes` - which contains a list of dimension\
        \ indices and this operator will insert a dimension of value `1` into the\
        \ corresponding index of the output tensor (`expanded`).\n\nFor example:\n\
        \  Given an input tensor (`data`) of shape [3, 4, 5], then\n  Unsqueeze(data,\
        \ axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data`\
        \ but with shape [1, 3, 4, 5, 1].\n\nThe input `axes` should not contain any\
        \ duplicate entries. It is an error if it contains duplicates.\nThe rank of\
        \ the output tensor (`output_rank`) is the rank of the input tensor (`data`)\
        \ plus the number of values in `axes`.\nEach value in `axes` should be within\
        \ the (inclusive) range [-output_rank , output_rank - 1].\nThe order of values\
        \ in `axes` does not matter and can come in any order.\n\n"
    arguments:
    - data
    - axes
    expression_string: onnx_ops.unsqueeze(data, axes)
onnx::Upsample:
    description: "\nUpsample the input tensor.\nEach dimension value of the output\
        \ tensor is:\n  output_dimension = floor(input_dimension * scale).\n"
    arguments:
    - X
    - scales
    expression_string: onnx_ops.upsample(X, scales, mode)
onnx::Where:
    description: '

        Return elements, either from X or Y, depending on condition.

        Where behaves like

        [numpy.where](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html)

        with three parameters.


        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).'
    arguments:
    - condition
    - X
    - Y
    expression_string: onnx_ops.where(condition, X, Y)
onnx::Xor:
    description: '

        Returns the tensor resulted from performing the `xor` logical operation

        elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting
        support).


        This operator supports **multidirectional (i.e., Numpy-style) broadcasting**;
        for more details please check [the doc](Broadcasting.md).

        '
    arguments:
    - A
    - B
    expression_string: onnx_ops.xor(A, B)
pattern_matching_function:
    description: Returns the productions that match the given goal and retrieval buffers.
    arguments:
    - productions
    - goal
    - retrieval
    expression_string: pattern_matching_function(productions,goal,retrieval)
pattern_to_string:
    description: Converts a pattern dictionary to a string format.
    arguments:
    - chunk
    expression_string: pattern_to_string(chunk)
retrieve_chunk:
    description: Retrieve a chunk from declarative memory given a pattern.
    arguments:
    - pattern
    - dm_chunks
    - types
    expression_string: retrieve_chunk(pattern,dm_chunks,types)
sin:
    description: Sine function
    arguments:
    - variable0
    - scale
    expression_string: scale * sin(variable0)
sinh:
    description: Hyperbolic sine function
    arguments:
    - variable0
    - scale
    expression_string: scale * sinh(variable0)
tan:
    description: Tangent function
    arguments:
    - variable0
    - scale
    expression_string: scale * tan(variable0)
tanh:
    description: Hyperbolic tangent function
    arguments:
    - variable0
    - scale
    expression_string: scale * tanh(variable0)
update_buffer:
    description: Returns a pattern to update the given buffer with.
    arguments:
    - production
    - buffer
    expression_string: update_buffer(production,buffer)
update_goal:
    description: Returns a pattern to update the goal buffer with.
    arguments:
    - production
    expression_string: update_goal(production)
update_retrieval:
    description: Returns a pattern to update the retrieval buffer with.
    arguments:
    - production
    expression_string: update_retrieval(production)
